{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8493af4b-b79f-47c6-966d-04935bb8efae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/home/ko20929/.conda/envs/sktime_latest/lib/python3.11/site-packages/antropy/fractal.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit((types.Array(types.float64, 1, \"C\", readonly=True), types.int32))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mne as mne\n",
    "import os \n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import constants\n",
    "from IPython.utils import io\n",
    "import time\n",
    "import sys\n",
    "import yasa\n",
    "from scipy.signal import welch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Import my modules\n",
    "import format_eeg_data\n",
    "import constants\n",
    "import eeg_stat_ts\n",
    "import run_expts\n",
    "\n",
    "#TS Fresh Parameter Settings\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "from tsfresh.feature_extraction import EfficientFCParameters\n",
    "from tsfresh.feature_extraction import extract_features\n",
    "\n",
    "# Set display options to show all rows and columns\n",
    "pd.set_option('display.max_rows', 50)  # Show rows\n",
    "pd.set_option('display.max_columns', 160)  # Show columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c3ae3a-9b5e-4109-a3db-8db535c5edf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14-Sep-23 12:16:26 | WARNING | Dependency not available for matrix_profile, this feature will be disabled!\n",
      "Feature Extraction: 100%|██████████| 119/119 [00:01<00:00, 73.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040\n",
      "1040\n",
      "1040\n",
      "N1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14-Sep-23 12:16:29 | WARNING | Dependency not available for matrix_profile, this feature will be disabled!\n",
      "Feature Extraction: 100%|██████████| 120/120 [00:02<00:00, 54.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040\n",
      "1040\n",
      "1038\n",
      "N2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14-Sep-23 12:16:33 | WARNING | Dependency not available for matrix_profile, this feature will be disabled!\n",
      "Feature Extraction: 100%|██████████| 120/120 [00:02<00:00, 48.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040\n",
      "1040\n",
      "1021\n",
      "N3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14-Sep-23 12:16:37 | WARNING | Dependency not available for matrix_profile, this feature will be disabled!\n",
      "Feature Extraction: 100%|██████████| 119/119 [00:02<00:00, 47.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040\n",
      "1040\n",
      "1017\n",
      "REM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14-Sep-23 12:16:41 | WARNING | Dependency not available for matrix_profile, this feature will be disabled!\n",
      "Feature Extraction: 100%|██████████| 120/120 [00:02<00:00, 54.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1040\n",
      "1040\n",
      "1030\n"
     ]
    }
   ],
   "source": [
    "load_path = '/user/home/ko20929/work/RBD_using_custom_package/Execute New Experiments/Baseline_Extensions/Gen_New_Features/generated_feats/'\n",
    "data_types = ['Wake','N1', 'N2', 'N3', 'REM']\n",
    "loaded_data = {}\n",
    "\n",
    "\n",
    "for data_type in data_types:\n",
    "    print(data_type)\n",
    "    X_y_groups = {}\n",
    "    \n",
    "    X = pd.read_hdf(load_path + data_type +  'six_second_max_freq_stats_df.h5', key='df', mode='r')\n",
    "    y = pd.read_hdf(load_path + data_type +  '_y.h5', key='df', mode='r') \n",
    "    groups = pd.read_hdf(load_path + data_type +  '_groups.h5', key='df', mode='r')\n",
    "    X, y , groups = X.reset_index(drop = True) , y.reset_index(drop = True) , groups.reset_index(drop = True)\n",
    "\n",
    "    #Transform the X into TS_Fresh Features___\n",
    "    # 1. Convert to TS_Fresh format Dataframe \n",
    "    ts_fresh_df = format_eeg_data.convert_sktime_df_to_ts_fresh_format(X, ts_cols = list(X.columns))\n",
    "    \n",
    "    # 2. Extract TS_Fresh Features from the dataframe\n",
    "    settings = MinimalFCParameters()\n",
    "    extracted_ts_fresh_df = extract_features(ts_fresh_df, column_id = 'id' , column_sort = 'time',  default_fc_parameters=settings)\n",
    "\n",
    "    # 3. Asign extract_ts_fresh_df to the variable X\n",
    "    X = extracted_ts_fresh_df.copy()\n",
    "    print(len(X.columns))\n",
    "    #Drop columns where all values are NA \n",
    "    X = X.dropna(axis = 1)\n",
    "    print(len(X.columns))\n",
    "\n",
    "    #Drop columns where all values are the same\n",
    "    # Find columns where all values are the same\n",
    "    same_value_columns = X.columns[X.nunique() == 1]\n",
    "    # Drop columns with the same values\n",
    "    X = X.drop(columns=same_value_columns)\n",
    "    print(len(X.columns))\n",
    "\n",
    "    #Generate the three dictionaries per data type _________________________________________________________________________________\n",
    "    #Generate regional features dict per data type \n",
    "    #Generate a region to features dictionary - this will enable us to run expts regionally as before\n",
    "    regional_features_dict = {}\n",
    "    region_channel_dict = constants.region_to_channel_dict\n",
    "    regions = list(region_channel_dict.keys())\n",
    "    for region in regions:\n",
    "        region_features = [col for col in X.columns if region + '_' in col]\n",
    "        if len(region_features) > 0 : \n",
    "            regional_features_dict[region] = region_features\n",
    "\n",
    "    # 2. #Create the combined regions dictionary\n",
    "    regions = list(regional_features_dict.keys())\n",
    "    combined_regions_features_dict = {}\n",
    "        \n",
    "    for i, region_1 in enumerate(regions):\n",
    "        for region_2 in regions[i+1:]:\n",
    "            new_key = region_1 + '_' + region_2\n",
    "            combined_regions_features_dict[new_key] = regional_features_dict[region_1] + regional_features_dict[region_2]\n",
    "\n",
    "    #3. Use all of the features\n",
    "    all_data_dict = {'All_regions' : list(X.columns) , 'All_regions_2' : list(X.columns)}\n",
    "       \n",
    "    X_y_groups['X'] = X\n",
    "    X_y_groups['y'] = y\n",
    "    X_y_groups['groups'] = groups\n",
    "    \n",
    "    X_y_groups['regions_dict'] = regional_features_dict\n",
    "    X_y_groups['combined_regions_dict'] = combined_regions_features_dict\n",
    "    X_y_groups['all_feats_dict'] = all_data_dict\n",
    "\n",
    "    loaded_data[data_type] = X_y_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be1be3-6472-4234-8642-82e2b6a20155",
   "metadata": {},
   "source": [
    "### Expt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31292933-d395-4591-858f-1f676ac71b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wake\n",
      "[ True  True  True ...  True  True  True]\n",
      "N1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Lengths must match to compare",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_type \u001b[38;5;129;01min\u001b[39;00m data_types:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data_type)\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mloaded_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloaded_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWake\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/sktime_latest/lib/python3.11/site-packages/pandas/core/ops/common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     79\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sktime_latest/lib/python3.11/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sktime_latest/lib/python3.11/site-packages/pandas/core/indexes/base.py:6761\u001b[0m, in \u001b[0;36mIndex._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6756\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[1;32m   6758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, (np\u001b[38;5;241m.\u001b[39mndarray, Index, ABCSeries, ExtensionArray)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m   6759\u001b[0m     \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   6760\u001b[0m ) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(other):\n\u001b[0;32m-> 6761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLengths must match to compare\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6763\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, ABCMultiIndex):\n\u001b[1;32m   6764\u001b[0m     other \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Lengths must match to compare"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "all_expt_results = {}\n",
    "\n",
    "for expt_num in [1,2,3,4] :\n",
    "    t3 = time.time()\n",
    "    expt_results = {}\n",
    "    for data_type in data_types:\n",
    "        X_y_groups = loaded_data[data_type]\n",
    "        \n",
    "        X = X_y_groups['X']\n",
    "        y = X_y_groups['y'] \n",
    "        groups = X_y_groups['groups'] \n",
    "            \n",
    "        #1.Generate expt specific X,y,groups\n",
    "        X_expt , y_expt , groups_expt, expt_info = run_expts.generate_expt_x_y_groups(X,y,groups, expt_num )\n",
    "    \n",
    "        results_df_regional = run_expts.run_mv_tsc(X_expt,y_expt,groups_expt,  {'RF' : RandomForestClassifier(random_state = 5) , 'DT' : DecisionTreeClassifier() , 'Ada_B' : AdaBoostClassifier(random_state = 5)} , return_df = True , subset_names_and_cols = X_y_groups['regions_dict'], random_states = [1,2] )\n",
    "        print('regional done...')\n",
    "        results_df_regions_combined = run_expts.run_mv_tsc(X_expt,y_expt,groups_expt,  {'RF' : RandomForestClassifier(random_state = 5) , 'DT' : DecisionTreeClassifier() , 'Ada_B' : AdaBoostClassifier(random_state = 5)} , return_df = True , subset_names_and_cols = X_y_groups['combined_regions_dict'] , random_states = [1,2] )\n",
    "        print('regions combined done...')\n",
    "        results_df_all_feats = run_expts.run_mv_tsc(X_expt,y_expt,groups_expt,  {'RF' : RandomForestClassifier(random_state = 5) , 'DT' : DecisionTreeClassifier() , 'Ada_B' : AdaBoostClassifier(random_state = 5)} , return_df = True , subset_names_and_cols = X_y_groups['all_feats_dict'], random_states = [1,2] )\n",
    "        \n",
    "        expt_results[data_type] = {'regional' : results_df_regional , 'regions_combined' : results_df_regions_combined , 'all_feats' : results_df_all_feats}\n",
    "    \n",
    "    all_expt_results[expt_num] = expt_results\n",
    "    #Saving Results \n",
    "    joblib.dump(all_expt_results, 'TS_Fresh_minimal_frequency_feats_results.pkl')\n",
    "    t4 = time.time()\n",
    "    loop_time = t4-t3\n",
    "    print(str(loop_time))\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "t2-t1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
