{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2232c392-f494-46bd-aaaf-3f568b72d01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/home/ko20929/.conda/envs/sktime_latest/lib/python3.11/site-packages/antropy/fractal.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  @jit((types.Array(types.float64, 1, \"C\", readonly=True), types.int32))\n",
      "/user/home/ko20929/.conda/envs/sktime_latest/lib/python3.11/site-packages/outdated/utils.py:14: OutdatedPackageWarning: The package yasa is out of date. Your version is 0.6.3, the latest is 0.6.5.\n",
      "Set the environment variable OUTDATED_IGNORE=1 to disable these warnings.\n",
      "  return warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mne as mne\n",
    "import os \n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import constants\n",
    "from IPython.utils import io\n",
    "import time\n",
    "import sys\n",
    "import yasa\n",
    "from scipy.signal import welch\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from scipy.signal import ShortTimeFFT\n",
    "from scipy.signal.windows import gaussian\n",
    "\n",
    "#Import my modules\n",
    "import format_eeg_data\n",
    "import constants\n",
    "import eeg_stat_ts\n",
    "\n",
    "#Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41cef7e0-1bba-415a-a840-0c47154d6379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7680"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256*30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0af081-a434-435f-9e80-98c6ec012839",
   "metadata": {},
   "source": [
    "#### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492fa1c6-da45-4c55-8366-f0130c37633e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N1\n",
      "Commencing Training ...\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/home/ko20929/.conda/envs/sktime_latest/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "\n",
      "Duration: 587 seconds\n",
      "Results Saved, on to next data type ...\n",
      "N3\n",
      "Commencing Training ...\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\n",
      "Duration: 3316 seconds\n",
      "Results Saved, on to next data type ...\n",
      "REM\n",
      "Commencing Training ...\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\n",
      "Duration: 2119 seconds\n",
      "Results Saved, on to next data type ...\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "#1.Load the data ________________________________________________________________________________________\n",
    "\n",
    "for data_type in ['N1', 'N3', 'REM', 'N2', 'Wake']:\n",
    "    df_list = joblib.load(data_type + '_normalised_dataframes.pkl')\n",
    "    \n",
    "    print(data_type)\n",
    "    \n",
    "    df_list = [ df.iloc[::2].reset_index(drop=True) for df in df_list]\n",
    "    \n",
    "    \n",
    "    # 1. generate all path names and class list(s) etc. \n",
    "    folder = '/user/home/ko20929/work/RBD_using_custom_package/Blue_pebble/'\n",
    "    paths = joblib.load(folder + data_type + '_paths.pkl') # keys : ['selected_paths', 's_class_list', 's_night_list', 's_sleep_type', 's_p_id']\n",
    "    \n",
    "    class_label_dict = {'HC': 0 , 'PD' : 1 , 'PD+RBD' : 2 , 'RBD' : 3} #Dictionary used to label the classes for reference\n",
    "    y = np.array([class_label_dict[c_name] for c_name in paths['s_class_list'] ] )\n",
    "    groups = paths['s_p_id']\n",
    "    \n",
    "    wake_dfs_binary = []\n",
    "    y_binary = []\n",
    "    groups_binary = []\n",
    "    \n",
    "    for df , class_label , group in zip(df_list, y, groups):\n",
    "        if class_label in [0,1]:\n",
    "            wake_dfs_binary.append(df)\n",
    "            y_binary.append(class_label)\n",
    "            groups_binary.append(group)\n",
    "    \n",
    "    y_binary = np.array(y_binary)\n",
    "\n",
    "#2. Generate 2 second segments of the data______________________________________________________________________ \n",
    "\n",
    "    #segment_length is user input and overlap is user input at the start of the script\n",
    "    segment_length = 256 \n",
    "    overlap = 0.5\n",
    "    \n",
    "    signal_slices = []\n",
    "    y_slice_labels = []\n",
    "    y_slice_groups = []\n",
    "    \n",
    "    for df, label, group in zip(wake_dfs_binary, y_binary, groups_binary):\n",
    "        \n",
    "        num_segments = int( np.floor(len(df)/segment_length) )\n",
    "        new_specs = []\n",
    "        \n",
    "        for i in np.arange(0,num_segments, 1 - overlap):\n",
    "            if i > num_segments - 1 :\n",
    "                # I don't want it to try to take an incomplete slice\n",
    "                # will be an incomplete slice causing errors downstream, needs to stop \n",
    "                \n",
    "                break\n",
    "              \n",
    "            start_index = int( np.floor(i*segment_length) )\n",
    "            end_index = start_index + segment_length\n",
    "            \n",
    "            slice_df = df.iloc[start_index : end_index, :].copy()        \n",
    "            signal_slices.append(slice_df)\n",
    "            \n",
    "            y_slice_labels.append(label)\n",
    "            y_slice_groups.append(group)\n",
    "            \n",
    "    plt.hist(y_slice_labels)\n",
    "\n",
    "#3. Select the correct channels ______________________________________________________________________________\n",
    "    neuro_headset_channels = [\"AF3\", \"F7\", \"F3\", \"FC5\", \"T7\", \"P7\", \"O1\", \"O2\", \"P8\", \"T8\", \"FC6\", \"F4\", \"F8\", \"AF4\"]\n",
    "\n",
    "    dif_channels = ['AF3', 'AF4'] #These channels are present in the neuro headset but not in my 57 EEG channels\n",
    "    channels_to_add = ['AF7' , 'AF8']  #These channels are  present in my 57 EEG Channels\n",
    "    channel_to_add_2 = ['Fp1' , 'Fp2'] #Alternatively I could add these two channels\n",
    "\n",
    "    channel_list = constants.channel_list\n",
    "    region_channel_dict = constants.region_to_channel_dict\n",
    "    \n",
    "    channel_to_region_dict = {}\n",
    "    for key, value in region_channel_dict.items():\n",
    "        for channel in value:\n",
    "            channel_to_region_dict[channel] = key\n",
    "\n",
    "    common_channels = []\n",
    "    dif_channels = []\n",
    "    for channel in neuro_headset_channels:\n",
    "        if channel in channel_list: \n",
    "            common_channels.append(channel)\n",
    "        else:\n",
    "            dif_channels.append(channel)\n",
    "    \n",
    "    channels_to_add = ['AF7' , 'AF8'] \n",
    "    \n",
    "    #Keep the channel ordering in line with original so append 'AF7' at the start and 'AF8' at the end\n",
    "    common_channels.insert(0, 'AF7')\n",
    "    common_channels.append('AF8')\n",
    "    channels_selected = common_channels.copy()   \n",
    "\n",
    "    # signal_slices, y_slice_labels, y_slice_groups\n",
    "    signal_slices_14_channels = [ df.loc[:, channels_selected].copy() for df in signal_slices ]\n",
    "    \n",
    "    # Make the slices an np array of the correct dimensions\n",
    "    signal_slices_14_channels_np = [df.T.values for df in signal_slices_14_channels]\n",
    "\n",
    "#4. Create the network from the paper\n",
    "    class PaperCNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Conv1d(14, 5, 20)\n",
    "            \n",
    "            self.bn1 = nn.BatchNorm1d(5)  # Batch norm after first conv layer\n",
    "            \n",
    "            self.conv2 = nn.Conv1d(5, 10, 10)\n",
    "    \n",
    "            self.conv3 = nn.Conv1d(10, 10, 10)\n",
    "            \n",
    "            self.conv4 = nn.Conv1d(10, 15, 5)       \n",
    "            \n",
    "            self.fc1 = nn.Linear(135,20)\n",
    "            self.fc2 = nn.Linear(20,10)\n",
    "            self.fc3 = nn.Linear(10,2)\n",
    "            \n",
    "        def forward(self, X):\n",
    "            X = F.relu(self.bn1(self.conv1(X))) #Relu and BN on first conv layer\n",
    "            X = F.max_pool1d(X,2) #first max pool layer\n",
    "            \n",
    "            X = F.relu(self.conv2(X)) #Relu on second conv layer\n",
    "            X = F.max_pool1d(X,2) # second max pool layer\n",
    "    \n",
    "            X = F.relu(self.conv3(X)) #Relu on third conv layer\n",
    "            X = F.max_pool1d(X,2) #Third Max pool layer\n",
    "    \n",
    "            X = F.relu(self.conv4(X)) #Relu on fourth conv layer\n",
    "            X = F.max_pool1d(X,2) #Fourth Max pool layer\n",
    "    \n",
    "            X = X.view(-1, 135) #Flatten the input\n",
    "    \n",
    "            X = F.relu(self.fc1(X) )\n",
    "            X = nn.Dropout(p=0.5)(X) # Dropout after first FC layer as in the paper\n",
    "    \n",
    "            X = F.relu(self.fc2(X) )\n",
    "            X = nn.Dropout(p=0.5)(X) # Dropout after first FC layer\n",
    "            \n",
    "            # X = F.softmax(self.fc3(X), dim = 1 ) \n",
    "            # Author uses softmax but doesn't state the loss function \n",
    "            # Author implies that loss function is cross entropy loss - however if we use this then we should remove the softmax from the network\n",
    "    \n",
    "            X = self.fc3(X) \n",
    "            \n",
    "            return X\n",
    "\n",
    "#5. Do Train Validation Splits\n",
    "    # Train and Validation splits only ----> NO TEST\n",
    "    # spectrogram_slices, y_slice_labels and y_slice_groups to work with\n",
    "    train_val_dict = {}\n",
    "    \n",
    "    for value in ['train' , 'val']:\n",
    "        train_val_dict[value] = {}\n",
    "    \n",
    "    X = np.stack(signal_slices_14_channels_np)\n",
    "    y = np.array(y_slice_labels)\n",
    "    groups = np.array( [int(group) for group in y_slice_groups] )\n",
    "    \n",
    "    gkf = GroupKFold(n_splits = 4) \n",
    "    fold = 0\n",
    "    \n",
    "    for train_index, val_index   in gkf.split(X, y, groups*1):\n",
    "        fold += 1\n",
    "        \n",
    "        X_train, y_train, groups_train  = X[train_index], y[train_index] , groups[train_index]\n",
    "        X_val, y_val, groups_val =  X[val_index], y[val_index] , groups[val_index]   \n",
    "        \n",
    "        train_val_dict['train'][fold] = X_train, y_train, groups_train\n",
    "        train_val_dict['val'][fold]   = X_val, y_val, groups_val\n",
    "        \n",
    "        total_len = len(X) \n",
    "        val_percent = 100*(len(X_val) / total_len)\n",
    "        train_percent = 100*(len(X_train) / total_len)\n",
    "        \n",
    "        # Commented out the printing here\n",
    "        # print('fold ' + str(fold) ) \n",
    "        # print( str(train_percent)[:3] + ' | '  + str(val_percent)[:3] + ' |' )\n",
    "    \n",
    "        # # testing that the splits are as expected\n",
    "        # print( np.unique(groups_train) )\n",
    "        # print( np.unique(groups_val) )\n",
    "        \n",
    "        # print('__________________________________________________________________________')\n",
    "    \n",
    "    #Output from this section of code is X_train, y_train, groups_train AND X_test, y_test, groups_test \n",
    "    import time\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    seeds = [2]\n",
    "    rows = len(seeds) # Make the figure the right size \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    fig = plt.figure(figsize=(24,4*rows),dpi=100)\n",
    "    \n",
    "    # k is for subplots within the overall figure \n",
    "    \n",
    "    k = 1\n",
    "    print('Commencing Training ...')\n",
    "    for fold in [1,2,3,4]:\n",
    "        print(fold)\n",
    "        X_train, y_train, groups_train = train_val_dict['train'][fold]\n",
    "        X_val, y_val, groups_val = train_val_dict['val'][fold]  \n",
    "        \n",
    "        # Creating train and test data loaders\n",
    "        train_data = [ (torch.from_numpy(input_slice).float().view(14,256), val) for input_slice, val in zip(X_train, y_train) ] \n",
    "        train_loader = DataLoader(train_data, batch_size=30, shuffle=True)\n",
    "        \n",
    "        val_data = [ (torch.from_numpy(input_slice).float().view(14,256), val) for input_slice, val in zip(X_val, y_val) ] \n",
    "        val_loader = DataLoader(val_data , batch_size=30, shuffle=False)\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for seed in [2,5,15,50]:\n",
    "        for seed in seeds:\n",
    "            # set all seeds \n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed) \n",
    "            \n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            # print(device) - to check that device is actually cuda\n",
    "            \n",
    "            model = model = PaperCNN()\n",
    "            model.to(device)\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr = 0.001 )\n",
    "    \n",
    "            epochs = 35\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            test_losses = []\n",
    "            \n",
    "            train_correct = []\n",
    "            val_correct = []\n",
    "            test_correct = []\n",
    "            \n",
    "            for i in range(epochs):\n",
    "                \n",
    "                trn_corr = 0\n",
    "                val_corr = 0\n",
    "                tst_corr = 0\n",
    "                 \n",
    "                \n",
    "                trn_loss = 0\n",
    "                val_loss = 0\n",
    "                tst_loss = 0\n",
    "                \n",
    "                model.train()\n",
    "                # Run the training batches\n",
    "                for b, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "                    b+=1\n",
    "            \n",
    "                    #Move train data to the GPU\n",
    "                    X_train_batch = X_train_batch.to(device)\n",
    "                    y_train_batch = y_train_batch.to(device)\n",
    "                    \n",
    "                    # Apply the model\n",
    "                    y_pred = model(X_train_batch)  # we don't flatten X-train here\n",
    "                    loss = criterion(y_pred, y_train_batch)\n",
    "             \n",
    "                    # Tally the number of correct predictions\n",
    "                    predicted = torch.argmax(y_pred.detach(),  dim = 1 ) \n",
    "    \n",
    "                    predicted = predicted.reshape(y_train_batch.shape)\n",
    "                    \n",
    "                    batch_corr = (predicted == y_train_batch).sum()\n",
    "                    trn_corr += batch_corr\n",
    "                    trn_loss += loss\n",
    "                    \n",
    "                    # Update parameters\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                train_losses.append(trn_loss)\n",
    "                train_correct.append(trn_corr)\n",
    "            \n",
    "                # Run the validation batches\n",
    "                # Some of the variables in this loop have the same name as the variables in the above loop... be aware of that plz!\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for b, (X_val_batch, y_val_batch) in enumerate(val_loader):\n",
    "                        b+=1\n",
    "                        \n",
    "                        #Move train data to the GPU\n",
    "                        X_val_batch = X_val_batch.to(device)\n",
    "                        y_val_batch = y_val_batch.to(device)\n",
    "            \n",
    "                        # Apply the model\n",
    "                        y_val = model(X_val_batch)\n",
    "            \n",
    "                        # Tally the number of correct predictions\n",
    "                        predicted = torch.argmax(y_val.detach(),  dim = 1 ) \n",
    "                        predicted = predicted.reshape(y_val_batch.shape)\n",
    "                        \n",
    "                        batch_corr = (predicted == y_val_batch).sum()\n",
    "                        val_corr += batch_corr\n",
    "            \n",
    "                        loss = criterion(y_val, y_val_batch)\n",
    "                        val_loss += loss \n",
    "                       \n",
    "                val_losses.append(val_loss)\n",
    "                val_correct.append(val_corr)\n",
    "            \n",
    "                \n",
    "               \n",
    "            \n",
    "            # Plot the outcome from the loop\n",
    "            \n",
    "            ax = fig.add_subplot(rows,4,k)\n",
    "            k+=1\n",
    "            plt.title('fold ' + str(fold), fontsize = 10)\n",
    "            plt.plot([(val.cpu() / len(X_train) ) for val in train_correct], label='training set accuracy')\n",
    "            plt.plot([(val.cpu()/len(X_val) ) for val in val_correct], label='validation set accuracy')\n",
    "            plt.ylabel('accuracy')\n",
    "            plt.xlabel('epochs') \n",
    "            plt.grid()\n",
    "        \n",
    "        \n",
    "        plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    plt.legend()   \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    #Add text at the bottom of the figure\n",
    "    # fig.text(0.5, 0, 'This is a caption at the bottom of the figure | Model : ' + str(model) , va='bottom')\n",
    "    fig.text(0.5, 0, f'\\nDuration: {time.time() - start_time:.0f} seconds' , ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout(pad = 2.0)\n",
    "    \n",
    "    save_name = data_type + '_' + 'Paper_CNN'\n",
    "    plt.savefig(save_name +'.png')\n",
    "        \n",
    "    \n",
    "    print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed  \n",
    "    print('Results Saved, on to next data type ...')\n",
    "    \n",
    "t2 = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
