{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e0e248-2af5-435d-9467-11cd73dbb793",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yasa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myasa\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m welch\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yasa'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mne as mne\n",
    "import os \n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import constants\n",
    "from IPython.utils import io\n",
    "import time\n",
    "import sys\n",
    "import yasa\n",
    "from scipy.signal import welch\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from scipy.signal import ShortTimeFFT\n",
    "from scipy.signal.windows import gaussian\n",
    "\n",
    "#Import my modules\n",
    "import format_eeg_data\n",
    "import constants\n",
    "import eeg_stat_ts\n",
    "\n",
    "#Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#Import Braindecode Model EEG Conformer\n",
    "from braindecode.models import EEGConformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a7dcd9e-302f-42a8-839e-c361b8bb10d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/home/ko20929/miniconda3/envs/sktime_latest/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b786eb-3285-4eb3-ab83-e58163e5a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuro_headset_channels = [\"AF3\", \"F7\", \"F3\", \"FC5\", \"T7\", \"P7\", \"O1\", \"O2\", \"P8\", \"T8\", \"FC6\", \"F4\", \"F8\", \"AF4\"]\n",
    "# These channels are equivalent to the neuro headset channels but for 'our' headset (There is code to generate these in notebook 18) \n",
    "eeg_14_channels = ['AF7', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF8']  \n",
    "\n",
    "num_channels = len(eeg_14_channels)\n",
    "num_channels\n",
    "\n",
    "# The below are currently in the script but need to be taken out to the top (here)\n",
    "segment_length = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3b6380-55b1-408d-839c-675b2a508193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m channels_selected \u001b[38;5;241m=\u001b[39m eeg_14_channels \u001b[38;5;66;03m# will later move this to the top of the notebook\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# signal_slices, y_slice_labels, y_slice_groups\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m signal_slices_14_channels \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_selected\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msignal_slices\u001b[49m\u001b[43m \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Make the slices an np array of the correct dimensions\u001b[39;00m\n\u001b[1;32m     73\u001b[0m signal_slices_14_channels_np \u001b[38;5;241m=\u001b[39m [df\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m signal_slices_14_channels]\n",
      "Cell \u001b[0;32mIn[3], line 70\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     67\u001b[0m channels_selected \u001b[38;5;241m=\u001b[39m eeg_14_channels \u001b[38;5;66;03m# will later move this to the top of the notebook\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# signal_slices, y_slice_labels, y_slice_groups\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m signal_slices_14_channels \u001b[38;5;241m=\u001b[39m [ \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_selected\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m signal_slices ]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Make the slices an np array of the correct dimensions\u001b[39;00m\n\u001b[1;32m     73\u001b[0m signal_slices_14_channels_np \u001b[38;5;241m=\u001b[39m [df\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m signal_slices_14_channels]\n",
      "File \u001b[0;32m~/.conda/envs/sktime_latest/lib/python3.11/site-packages/pandas/core/generic.py:6452\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m   6342\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   6343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[1;32m   6344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6345\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[1;32m   6346\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6450\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[1;32m   6451\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 6452\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[1;32m   6454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/sktime_latest/lib/python3.11/site-packages/pandas/core/internals/managers.py:653\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    651\u001b[0m     new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m--> 653\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcopy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m res\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m new_axes\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sktime_latest/lib/python3.11/site-packages/pandas/core/internals/managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    355\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/.conda/envs/sktime_latest/lib/python3.11/site-packages/pandas/core/internals/blocks.py:540\u001b[0m, in \u001b[0;36mBlock.copy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    538\u001b[0m refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 540\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    541\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkEAAAGgCAYAAACt9LMXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAu+klEQVR4nO3de3BUZZ7/8U8nnQ6dhCGBhdRCAYkCq1EXGlBEkISpUtT6xRIvKyuXAQlyEYagYVfUqVplggO73rhU0IGZaDDL4AhrwQy7q2gQd2VYSDJBYUfRdGBlJAgIm3RI53J+f/Dr86NJgKTT6dB53q8qK3We/vbTz/kScz45p/vEYVmWJQAAAMPEdPUCAAAAugIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASM6uXsC1rKysTJZlKS4urquXAgAA2qihoUEOh0Mej+eKdZwJugLLstQZ95K0LEt+v79T5kYweh0Z9Dky6HNk0OfI6Mw+t/X4zZmgKwicAbrlllvCOq/P59Phw4c1ZMgQJSQkhHVuBKPXkUGfI4M+RwZ9jozO7PPBgwfbVMeZIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEFdJC4uTg6Ho6uXAQCAsZxdvQATORwO3Zxxk2KcsV29lHazmpvliCE7AwCiHyGoi8Q4Y+XftEPWiVNdvZQ2c6T2kWva/+nqZQAAEBaEoC5knTgl69sTXb0MAACMxHUNAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADCSsz3Ff/jDH7Rs2TINGDAgaPzOO+/UE088YW9v27ZNRUVFcrvdqqur06xZs5SdnR30HL/frzVr1mjPnj1yu92KjY3VM888o5tvvjmorrq6WitWrNDRo0clSenp6Xr22WfVp0+foLo//vGPWrlypSzLks/nU1ZWlhYtWiSns127CAAADNHuhDB58mQtWrToso9v375dL774orZu3ar09HR9/fXXevjhhxUfH6+7777brvv5z3+uiooKFRcXKyEhQdu2bdPMmTO1bds2DRw4UNKFoDR79mx5PB5t3bpVkrRs2TLNmTNHW7ZssQNOZWWlZs6cqRdffFHZ2dmqra3VlClTVFtbq+eff769uwgAAAwQ1sthlmXp1VdfVXZ2ttLT0yVJ119/ve655x69/PLLdt3Ro0e1ZcsW5eTkKCEhQdKFcJWcnKw333zTrtu+fbu+/PJLPfnkk/bYokWL9MUXX2jnzp322Pr169WvXz/7bFNiYqJmz56t4uJiHT9+PJy7CAAAuomwXiv66quv9O2338rj8QSNjxw5Ulu3blVlZaXS09P1ySefyLKsFnUjRozQxx9/bG/v3r1bAwYMUGpqqj3Wv39/paamqqSkxA49JSUl+vGPfxw0l8fjUVNTk/bs2aNHH3005H0KXF4LJ7/fL7fbHdY5I6murk6WZXX1Mtqkrq4u6Cs6B32ODPocGfQ5Mjqzz5ZlyeFwXLWu3SGovLxcc+bMkc/nk9Pp1B133KGf/OQn6tGjh6qqqiRJ/fr1C3pOYNvr9So9PV1er7fVutTUVJ08eVK1tbVKTEyU1+ttUROoq6yslCSdPXtWP/zwQ6tzBV6zIxoaGnT48OEOzXEpt9ut5OTksM4ZSZWVlVH3w6Gj3wdoG/ocGfQ5MuhzZHRWn10u11Vr2hWCevbsqdTUVC1dulQpKSk6fvy45s+fr3/7t3/Tb37zG9XW1rb6woHtwBkVn88nh8OhuLi4y9YlJibK5/O1GhZcLpdOnToVNOfl5gqsKVRxcXEaMmRIh+a4lN/vD+t8kZaenh5VZ4K8Xq/S0tKi+uzbtY4+RwZ9jgz6HBmd2ecjR460qa5dISgjI0MrVqywt/v376+nn35ac+bM0QcffKDExERJLQ/yge3A+38SEhJkWZYaGhqCwktrda0FBr/fH1QjXThj09prBtYUKofDYb9GuLTlFN21LBp/KLjd7rD/O6Il+hwZ9Dky6HNkdEaf23qc7fAbowNvgD527JgGDx4s6cLH2i8W2E5LSwv62lpd37597eCSlpbWoiZQF3jdXr16KTk5+aqvCQAAcLF2haCXX35Zx44dCxr77rvvJF14D87QoUM1YMAAlZWVBdWUlZUpLS3NDi4TJkyQw+FQeXl5UF15ebmysrLs7QkTJujbb78NCjh//vOf9d133ykzM9Mey8zMbPGapaWliomJ0Z133tmeXQQAAIZoVwgqLy9XYWGhmpqaJEk1NTVat26dBgwYoLvuuksOh0NLlizRjh077Dc6ff3119q5c6eeeuope55BgwbpkUce0caNG+032L7//vs6ffq05s6da9fdf//9Gjp0qNatW2ePrV27VhkZGbrvvvvssfnz5+vEiRP63e9+J+nC+4B+/etfa+rUqerfv387WwIAAEzQrvcEzZ07V1u2bNGUKVMUHx8vn8+nW265Rf/4j/9oX8LKzs5WQ0ODlixZooSEBPl8Pr3wwguaNGlS0Fw/+9nPtHr1ak2ZMkUJCQmKjY1VYWGhfaNE6cKbm3/1q19pxYoVevDBByVduPy2YcOGoDtBp6enq7CwUCtXrtSmTZuC7hgNAADQmnaFoPHjx2v8+PFXrXvwwQft0HI5LpdLeXl5ysvLu2Jdv3799Nprr131NYcPH67i4uKr1gEAAEj8AVUAAGAoQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADCSM9Qnnjt3TtnZ2YqNjdVHH30U9Nju3bu1Zs0axcfHq7a2Vg888IBmzpzZYo4NGzZox44dSkxMlN/vV25ursaNGxdUU1NTo1WrVungwYOKi4tTSkqKnnvuOQ0aNCiorrKyUvn5+Tp37pz8fr88Ho/y8vKUmJgY6i4CAIBuLOQQ9MILL+j8+fMtQsb+/fv15JNPqrCwUKNHj9bJkyc1efJkSQoKQm+88YaKi4u1detW9enTR3v37tWcOXO0adMmDR8+3K5bvHixYmJi9O6778rpdGrt2rWaMWOGtm/frp49e0qSzpw5o+nTp2vatGmaN2+eGhsb9cQTTygvL08FBQWh7iIAAOjGQroc9q//+q86e/asJk6c2OKx1157TWPGjNHo0aMlSX379tWUKVO0Zs0anT9/XpJUW1ur9evX67HHHlOfPn0kSbfffrs8Ho9ef/11e669e/fq008/1YIFC+R0XshrOTk5Onv2rN555x27rqioSHV1dXr88cclSU6nU/Pnz9dHH32k0tLSUHYRAAB0c+0+E3Ty5Em98sorKioq0quvvhr0WE1Njfbv36+FCxcGjY8cOVJr1qzR/v37NX78eO3bt08+n08ejyeozuPx6Je//KXq6urkdru1e/duOZ1O3XLLLXZNjx49dMMNN6ikpETz5s2TJJWUlCgjI0Mul8uuGz58uGJiYlRSUqKRI0e2dzdtlmXJ5/OF/PzW+P1+ud3usM4ZSXV1dbIsq6uX0SZ1dXVBX9E56HNk0OfIoM+R0Zl9tixLDofjqnXtDkHPP/+8Fi1apNTU1BaPHT16VJZlqV+/fkHjgVqv16vx48erqqpKklqta2pq0rFjxzRs2DB5vV717t3bPgt0cd1nn31mb1dVVSkrKyuoxuVyKSUlRV6vt727GKShoUGHDx/u0ByXcrvdSk5ODuuckVRZWRl1Pxw6+n2AtqHPkUGfI4M+R0Zn9fniEyOX064QtGXLFsXHxys7O7vVxwNnTC594cB24PHa2to21fl8vlZ3wuVyBZ2duVJd4LVCFRcXpyFDhnRojkv5/f6wzhdp6enpUXUmyOv1Ki0tLarPvl3r6HNk0OfIoM+R0Zl9PnLkSJvq2hyCjh07pg0bNmjz5s2XrUlISJDU8iAf2A48Hngz9dXqEhISWg0Mfr/frrlaXUc/HeZwOIJeKxzacoruWhaNPxTcbnfY/x3REn2ODPocGfQ5Mjqjz209zrY5BH388ceKj4/X4sWL7bFvvvlG586d0/Tp0yVJBQUFcjgcqq6uDnpuYDstLU2SNHjwYHs8MBbYjo2N1cCBA+36PXv2qLGxMeiSWHV1tdLT0+3twYMHt3hNv9+vM2fOBM0PAAAQ0OZPhwU+ll5UVGT/d+edd6pv3772dlJSkkaNGqWysrKg55aWliopKcn+xNhtt90mt9ut8vLyoLqysjKNGTPGPtMwYcIENTQ06PPPP7dr6uvrdfjwYWVmZtpjmZmZOnToUNDZoIqKCjU3NwfVAQAABIT9jtG5ubnat2+fDhw4IEn6/vvvtXnzZi1cuFA9evSQdOFy2Lx581RcXKzTp09Lkvbt26fS0lLl5ubac40dO1bjxo1TQUGBmpqaJEkbN25Ur169NG3aNLtuxowZcrvdKiwslCQ1NjaqoKBAEydO1KhRo8K9iwAAoBsI6WaJH3zwgd5+++2gy2FjxozRwoULdeutt2rt2rV66aWX7DtGz549u8Udo+fOnSun06lZs2YpKSlJfr9fBQUFQTdKlKTVq1dr5cqVeuihh+RyuZScnKy33nrLvlGiJKWkpOjtt99Wfn6+du3apfr6eo0YMUJLly4NZfcAAIABQgpBd911l+66667LPp6ZmXnVy1AOh0M5OTnKycm5Yl1SUpKWL19+1TVdd9112rhx41XrAAAAJP6AKgAAMBQhCAAARJzD4VBcXFyXroEQBABAN2A1N3f1EtrF7Xbr5oybuvTeeSH/FXkAAHDtcMTEyL9ph6wTp7p6KW3iSO0j17T/IzV03RoIQQAAdBPWiVOyvj3R1cuIGlwOAwAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRne4orKir0z//8z6qqqpLT6dTZs2c1aNAg5ebm6vrrr7frtm3bpqKiIrndbtXV1WnWrFnKzs4Omsvv92vNmjXas2eP3G63YmNj9cwzz+jmm28OqquurtaKFSt09OhRSVJ6erqeffZZ9enTJ6juj3/8o1auXCnLsuTz+ZSVlaVFixbJ6WzXLgIAAEO0KyHs3LlTfr9fRUVFio2NVWNjoxYvXqxZs2Zp9+7dcjgc2r59u1588UVt3bpV6enp+vrrr/Xwww8rPj5ed999tz3Xz3/+c1VUVKi4uFgJCQnatm2bZs6cqW3btmngwIGSLgSl2bNny+PxaOvWrZKkZcuWac6cOdqyZYsdcCorKzVz5ky9+OKLys7OVm1traZMmaLa2lo9//zz4eoVAADoRtp1OeyRRx7RsmXLFBsbK0lyOp0aM2aMTpw4oZqaGlmWpVdffVXZ2dlKT0+XJF1//fW655579PLLL9vzHD16VFu2bFFOTo4SEhIkSZMnT1ZycrLefPNNu2779u368ssv9eSTT9pjixYt0hdffKGdO3faY+vXr1e/fv3ss02JiYmaPXu2iouLdfz48fb2BAAAGKBdZ4Kuu+66oO1jx47pvffe09SpU9WzZ099+eWX+vbbb+XxeILqRo4cqa1bt6qyslLp6en65JNPZFlWi7oRI0bo448/trd3796tAQMGKDU11R7r37+/UlNTVVJSYoeekpIS/fjHPw6ay+PxqKmpSXv27NGjjz7ant0MEri8Fk5+v19utzusc0ZSXV2dLMvq6mW0SV1dXdBXdA76HBn0OTKisc8OhyNqjyv19fVhP6ZYliWHw3HVupDeMFNSUqJVq1bp2LFjysnJ0U9/+lNJUlVVlSSpX79+QfWBba/Xq/T0dHm93lbrUlNTdfLkSdXW1ioxMVFer7dFTaCusrJSknT27Fn98MMPrc4VeM2OaGho0OHDhzs0x6XcbreSk5PDOmckVVZWRtUPB6nj3wdoG/ocGfQ5MqKpz263WxkZGV29jJAcP368U44pLpfrqjUhhaCsrCxlZWXpm2++0cKFC/X1119r9erVqq2tbfWFA9uBMyo+n08Oh0NxcXGXrUtMTJTP52s1LLhcLp06dSpozsvNFVhTqOLi4jRkyJAOzXEpv98f1vkiLT09ParOBHm9XqWlpUXtb0nRgD5HBn2OjGjsc1vOelyr+vfv36bA0h5HjhxpU12HPjp13XXXKS8vT/Pnz9eePXuUmJgoqeVBPrAdeP9PQkKCLMtSQ0NDUHhpra61wOD3+4NqpAtnbFp7zcCaQuVwOOzXCJdo/maVFDU/FC7mdrvD/u+IluhzZNDnyKDPkREfHx/240pbj7PtemN0a4Fk6NChkqT//u//1uDBgyVd+Fj7xQLbaWlpQV9bq+vbt68dXNLS0lrUBOoCb7zu1auXkpOTr/qaAAAAF2tXCLrnnnvsy1ABJ06ckCQlJydr6NChGjBggMrKyoJqysrKlJaWZgeXCRMmyOFwqLy8PKiuvLxcWVlZ9vaECRP07bffBgWcP//5z/ruu++UmZlpj2VmZrZ4zdLSUsXExOjOO+9szy4CAABDtPuO0QUFBWpqapIk1dTUaPXq1erbt6/uvvtuORwOLVmyRDt27LDfUPb1119r586deuqpp+w5Bg0apEceeUQbN2603wz1/vvv6/Tp05o7d65dd//992vo0KFat26dPbZ27VplZGTovvvus8fmz5+vEydO6He/+52kC+8D+vWvf62pU6eqf//+7d1FAABggHa9JygvL0/btm3TI488IrfbrdraWt14441asWKFevXqJUnKzs5WQ0ODlixZooSEBPl8Pr3wwguaNGlS0Fw/+9nPtHr1ak2ZMkUJCQmKjY1VYWGhfaNE6cKbm3/1q19pxYoVevDBByVdeFPuhg0bgu4EnZ6ersLCQq1cuVKbNm0KumM0AABAa9oVgu67776gMzCX8+CDD9qh5XJcLpfy8vKUl5d3xbp+/frptddeu+prDh8+XMXFxVetAwAAkPgDqgAAwFCEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRne4r/8Ic/aPPmzTp58qQsy1JNTY3uvvtuzZ49Wz169LDrdu/erTVr1ig+Pl61tbV64IEHNHPmzBbzbdiwQTt27FBiYqL8fr9yc3M1bty4oJqamhqtWrVKBw8eVFxcnFJSUvTcc89p0KBBQXWVlZXKz8/XuXPn5Pf75fF4lJeXp8TExPbsIgAAMES7QtDzzz+ve++9V6+88oocDoe8Xq/+5m/+Rl9++aVef/11SdL+/fv15JNPqrCwUKNHj9bJkyc1efJkSQoKQm+88YaKi4u1detW9enTR3v37tWcOXO0adMmDR8+3K5bvHixYmJi9O6778rpdGrt2rWaMWOGtm/frp49e0qSzpw5o+nTp2vatGmaN2+eGhsb9cQTTygvL08FBQUd7REAAOiG2nU5bNiwYcrJyZHD4ZAkpaWl6d5779W///u/q7a2VpL02muvacyYMRo9erQkqW/fvpoyZYrWrFmj8+fPS5Jqa2u1fv16PfbYY+rTp48k6fbbb5fH47HDlCTt3btXn376qRYsWCCn80Jey8nJ0dmzZ/XOO+/YdUVFRaqrq9Pjjz8uSXI6nZo/f74++ugjlZaWhtQYAADQvbUrBK1bt04/+tGPgsZ69Oghh8Oh2NhY1dTUaP/+/fJ4PEE1I0eOtB+TpH379snn87Wo83g82rt3r+rq6iRduKzmdDp1yy23BL3eDTfcoJKSEnuspKREGRkZcrlc9tjw4cMVExMTVAcAABDQrsthrfmv//ovTZo0ST169NChQ4dkWZb69esXVJOamipJ8nq9Gj9+vKqqqiSp1bqmpiYdO3ZMw4YNk9frVe/eve2zQBfXffbZZ/Z2VVWVsrKygmpcLpdSUlLk9Xo7tH+WZcnn83Vojkv5/X653e6wzhlJdXV1siyrq5fRJoFAHfiKzkGfI4M+R0Y09tnhcETtcaW+vj7sxxTLsuyrVlfSoRD0+9//XidOnNAbb7whSXZYuPiMzMXbgccDl86uVufz+VrUBOouDiZXqgu8VqgaGhp0+PDhDs1xKbfbreTk5LDOGUmVlZVR9cNBUofDMNqGPkcGfY6MaOqz2+1WRkZGVy8jJMePH++UY0prueBSIYegiooKrVq1Shs2bFDfvn0lSQkJCZIunOm4WGA78HjgE1tXq0tISGhRE6gL1FytrqOfDouLi9OQIUM6NMelWltrNElPT4+qM0Fer1dpaWlR+1tSNKDPkUGfIyMa+9yWsx7Xqv79+7cpsLTHkSNH2lQXUgiqqKjQ0qVLVVBQoBtvvNEeHzRokBwOh6qrq4PqA9tpaWmSpMGDB9vjgbHAdmxsrAYOHGjX79mzR42NjUGXxKqrq5Wenm5vDx48uMVr+v1+nTlzJmj+UDgcjqDAFQ7R/M0qKWp+KFzM7XaH/d8RLdHnyKDPkUGfIyM+Pj7sx5W2HmfbfbPEAwcO6O/+7u+0bt06OwDt3LlTx44dU1JSkkaNGqWysrKg55SWliopKcn+xNhtt90mt9ut8vLyoLqysjKNGTPGbsaECRPU0NCgzz//3K6pr6/X4cOHlZmZaY9lZmbq0KFDQWdYKioq1NzcHFQHAAAQ0K4QtHfvXi1cuFCLFi1SXV2dDh48qIMHD+r999/X8ePHJUm5ubnat2+fDhw4IEn6/vvvtXnzZi1cuNC+oWJiYqLmzZun4uJinT59WtKFT4yVlpYqNzfXfr2xY8dq3LhxKigoUFNTkyRp48aN6tWrl6ZNm2bXzZgxQ263W4WFhZKkxsZGFRQUaOLEiRo1alRonQEAAN1auy6HLVmyRKdPn9ZTTz3V4rFZs2ZJkm699VatXbtWL730kn3H6NmzZ7e4Y/TcuXPldDo1a9YsJSUlye/3q6CgIOhGiZK0evVqrVy5Ug899JBcLpeSk5P11ltv2TdKlKSUlBS9/fbbys/P165du1RfX68RI0Zo6dKl7dk9AABgkHaFoIs/ln4lmZmZV70M5XA4lJOTo5ycnCvWJSUlafny5Vd9zeuuu04bN25s0/oAAAD4A6oAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkZ6hP/PDDD7V8+XKNHTtWv/jFL1o8vnv3bq1Zs0bx8fGqra3VAw88oJkzZ7ao27Bhg3bs2KHExET5/X7l5uZq3LhxQTU1NTVatWqVDh48qLi4OKWkpOi5557ToEGDguoqKyuVn5+vc+fOye/3y+PxKC8vT4mJiaHuJgAA6KbaHYLq6uqUl5cnt9uthoaGVmv279+vJ598UoWFhRo9erROnjypyZMnS1JQEHrjjTdUXFysrVu3qk+fPtq7d6/mzJmjTZs2afjw4Xbd4sWLFRMTo3fffVdOp1Nr167VjBkztH37dvXs2VOSdObMGU2fPl3Tpk3TvHnz1NjYqCeeeEJ5eXkqKCho724CAIBurt2Xw86fP6+pU6fqn/7pn9SjR49Wa1577TWNGTNGo0ePliT17dtXU6ZM0Zo1a3T+/HlJUm1trdavX6/HHntMffr0kSTdfvvt8ng8ev311+259u7dq08//VQLFiyQ03khs+Xk5Ojs2bN655137LqioiLV1dXp8ccflyQ5nU7Nnz9fH330kUpLS9u7mwAAoJtrdwhKSUnRHXfccdnHa2pqtH//fnk8nqDxkSNH2o9J0r59++Tz+VrUeTwe7d27V3V1dZIuXFZzOp265ZZb7JoePXrohhtuUElJiT1WUlKijIwMuVwue2z48OGKiYkJqgMAAJA68J6gyzl69Kgsy1K/fv2CxlNTUyVJXq9X48ePV1VVlSS1WtfU1KRjx45p2LBh8nq96t27t30W6OK6zz77zN6uqqpSVlZWUI3L5VJKSoq8Xm/I+2NZlnw+X8jPb43f75fb7Q7rnJFUV1cny7K6ehltEgjTga/oHPQ5MuhzZERjnx0OR9QeV+rr68N+TLEsSw6H46p1YQ9BgcBw8RmZi7cDj9fW1rapzufztagJ1F0cTq5UF3itUDQ0NOjw4cMhP781brdbycnJYZ0zkiorK6Pqh4OkDgVhtB19jgz6HBnR1Ge3262MjIyuXkZIjh8/3inHlNYywaXCHoISEhIkXTjbcbHAduDxwCe2rlaXkJDQoiZQF6i5Wl1HPh0WFxenIUOGhPz81rS2zmiSnp4eVWeCvF6v0tLSova3pGhAnyODPkdGNPa5LWc9rlX9+/dvU2BpjyNHjrSpLuwhaNCgQXI4HKqurg4aD2ynpaVJkgYPHmyPB8YC27GxsRo4cKBdv2fPHjU2NgZdEquurlZ6erq9PXjw4Bav6ff7debMmaD528vhcASFrXCI5m9WSVHzQ+Fibrc77P+OaIk+RwZ9jgz6HBnx8fFhP6609Tgb9pslJiUladSoUSorKwsaLy0tVVJSkv2Jsdtuu01ut1vl5eVBdWVlZRozZozdkAkTJqihoUGff/65XVNfX6/Dhw8rMzPTHsvMzNShQ4eCzrJUVFSoubk5qA4AAEDqpDtG5+bmat++fTpw4IAk6fvvv9fmzZu1cOFC+2P1iYmJmjdvnoqLi3X69GlJFz4xVlpaqtzcXHuusWPHaty4cSooKFBTU5MkaePGjerVq5emTZtm182YMUNut1uFhYWSpMbGRhUUFGjixIkaNWpUZ+wmAACIYiFdDnvuued09OhRnTx5Unv27NH06dM1adIkO5TceuutWrt2rV566SX7jtGzZ89uccfouXPnyul0atasWUpKSpLf71dBQUHQjRIlafXq1Vq5cqUeeughuVwuJScn66233rJvlChd+Oj+22+/rfz8fO3atUv19fUaMWKEli5dGsouAgCAbi6kEJSfn3/VmszMzKtehnI4HMrJyVFOTs4V65KSkrR8+fKrvuZ1112njRs3XrUOAACAP6AKAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGIgQBAAAjEYIAAICRCEEAAMBIhCAAAGAkQhAAADASIQgAABiJEAQAAIxECAIAAEYiBAEAACMRggAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgJEIQAAAwEiEIAAAYiRAEAACMRAgCAABGcnb1AsKpsrJS+fn5OnfunPx+vzwej/Ly8pSYmNjVSwMAANeYbnMm6MyZM5o+fbpGjx6tLVu26Le//a2qqqqUl5fX1UsDAADXoG4TgoqKilRXV6fHH39ckuR0OjV//nx99NFHKi0t7eLVAQCAa023CUElJSXKyMiQy+Wyx4YPH66YmBiVlJR03cIAAMA1qdu8J6iqqkpZWVlBYy6XSykpKfJ6vSHN2dDQIMuyVFFR0fEFXsSyLMXExMi64wapaVhY5+5UsTFyHDwoy7K6eiVtZlmWHA6HvvrqKzkcjq5eTrdFnyODPkdGtPbZ4XBE13Hl/x1Tmpubw97nhoaGNs3ZbUKQz+cLOgsU4HK5VFtbG9KcgQaG+x/HnjcpIazzRkq0/VCIiek2JzyvWfQ5MuhzZERzn6PxuNIZvXY4HGaFoISEBPn9/hbjfr8/5E+HeTyeji4LAABco6Iz6rZi8ODBqq6uDhrz+/06c+aM0tLSumZRAADgmtVtQlBmZqYOHToUdDaooqJCzc3NyszM7MKVAQCAa1G3CUEzZsyQ2+1WYWGhJKmxsVEFBQWaOHGiRo0a1bWLAwAA1xyHFU0f9bmKb775Rvn5+aqpqVF9fb1GjBihpUuXcsdoAADQQrcKQQAAAG3VbS6HAQAAtAchCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASN3mD6heKyorK5Wfn69z587J7/fL4/EoLy+vTTds3LBhg3bs2KHExET5/X7l5uZq3LhxEVh1dAql12fPntVvfvMblZSUyOl0qra2Vr1799bChQs1fPjwCK4+enTkezpg48aNWrVqlV566SU9+OCDnbja6NWRPpeWlqqgoEB+v18//PCDLMvS1KlT9eijj0Zg5dEl1D4fPXpUL7/8so4dO6bExET5fD49/PDD+tu//dsIrTw6ffjhh1q+fLnGjh2rX/ziF216TkSPhRbC5vTp09a4ceOsgoICy7Isq6GhwZo1a5Y1b968qz53/fr11oQJE6zvv//esizL+uyzz6ybb77ZKi8v79Q1R6tQe/0v//Iv1rhx46z/+Z//sSzLspqbm63ly5dbN910k3Xo0KFOX3e06cj3dMCf/vQna9y4cdawYcOs9957r7OWGtU60uf//M//tLKysqxvvvnGHvv5z39uPf3005223mjVkT7fdddd1ty5c62GhgbLsiyrqqrKGjFihPXb3/62U9ccrXw+n7VgwQLr6aeftsaOHWv9/d//fZueF+ljISEojF5//XVr5MiRVn19vT22b98+a9iwYdaBAwcu+7yamhprxIgR1vr164PGp0+fbs2aNavT1hvNQu11SUmJ9ctf/jJo7OTJk9awYcOslStXdtp6o1WofQ7w+/3W5MmTrffff58QdAWh9rm5udm66667rKKioqDxU6dOEepbEWqfz5w5Yw0bNszatGlT0PjkyZOtBQsWdNp6o9np06et//iP/7Asy7ImTpzYphDUFcdC3hMURiUlJcrIyJDL5bLHhg8frpiYGJWUlFz2efv27ZPP55PH4wka93g82rt3r+rq6jpryVEr1F5nZmYqJycnaKxHjx6SJKeTq8OXCrXPAWvXrtXYsWM1cuTITlxl9Au1zxUVFaqqqtIdd9wRNN67d2/deOONnbXcqBVqn5OTk3XnnXdq586d+t///V9JUnl5ub766iv17du3s5cdlVJSUlp8X15NVxwLCUFhVFVVpX79+gWNuVwupaSkyOv1XvF5klo8NzU1VU1NTTp27FjY1xrtQu11a/bt26eYmBhlZ2eHcYXdQ0f6XF5erpKSEi1evLgTV9g9hNrnw4cPS5JOnDih+fPna8qUKZo5c6Y2b96s5ubmzlxyVOrI93NBQYHS0tI0YcIE3XvvvZoyZYr++q//WgsXLuzEFZulK46F/OobRj6fL+g3jACXy6Xa2trLPi/w2KXPDWz7fL4wrrJ7CLXXl2poaNDrr7+uBQsWaOjQoeFcYrcQap/r6ur0/PPPa9WqVa0+H8FC7fMPP/wgSVqxYoXeeOMN9e/fX1988YVmzpypyspKLVu2rLOWHJVC7bNlWVq0aJFOnTqlXbt2qXfv3vrTn/6kDz74oF0fEMCVdcWxkDNBYZSQkCC/399i3O/3X/F/lMBjlz43sJ2QkBDGVXYPofb6Ys3NzXrmmWd000038dvcZYTa51WrVum+++5TRkZGZy6v2wi1zzExF36ET5s2Tf3795ck3XTTTXr44Yf11ltvqaampnMWHKVC7fPHH3+sjz/+WD/96U/Vu3dvSdJf/dVfyev1asmSJZ22XtN0xbGQM0FhNHjwYFVXVweN+f1+nTlzRmlpaVd8niRVV1cH1VVXVys2NlYDBw7sjOVGtVB7HdDU1KRnn31WiYmJ+od/+Ac5HI5OWml0C7XPn3zyiVJTU/XZZ59Jkurr6yVJb775prZt26bJkyfzUfmLhNrnQPAJfA0YNGiQLMtSVVWVbrrpprCvN1qF2udvvvlG0oW+Xjrf2rVrVVNTo6SkpLCv1zRdcSzkTFAYZWZm6tChQ0EptqKiQs3NzcrMzLzs82677Ta53W6Vl5cHjZeVlWnMmDFyu92dteSoFWqvJamxsVFPP/20fvSjH+nFF19UTEyMff8gBAu1z7t27VJxcbGKiopUVFSkV155RZL0xBNPqKioiAB0iVD7PGbMGMXGxuq7774LGg8c6P/iL/6icxYcpULt81/+5V9KUosA9d133ykuLo5LvmHSFcdCQlAYzZgxQ263W4WFhZIuHGwLCgo0ceJEjRo1yq5btmyZsrOz7d+OExMTNW/ePBUXF+v06dOSLrxZt7S0VLm5uZHejagQaq/9fr8WL14sn8+n+++/XwcPHtTBgwd14MAB7dixoyt25ZoWap/RPqH2uW/fvnrsscf0zjvv6Ny5c5IuvEn6vffe0/3336/U1NSI78u1LNQ+Z2ZmasCAAXrzzTftAHXkyBH9/ve/16RJkwhBIboWjoVcDgujlJQUvf3228rPz9euXbtUX1+vESNGaOnSpUF19fX1On/+vCzLssfmzp0rp9OpWbNmKSkpSX6/XwUFBdzF+DJC7fW7776rDz/8UJK0e/fuoNrbbrstMouPIh35ng6YP3++Tp06Jen/Xw5buXJli0s4JutIn5ctW6Z169Zp6tSp6tmzp/x+v6ZPn66f/OQnkd6Na16ofU5KStJbb72lV199VY8++qh69OihmpoazZgxQ3Pnzu2KXYkKzz33nI4ePaqTJ09qz549mj59uiZNmqRp06ZJujaOhQ6rtZ9aAAAA3RyXwwAAgJEIQQAAwEiEIAAAYCRCEAAAMBIhCAAAGIkQBAAAjEQIAgAARiIEAQAAIxGCAACAkQhBAADASIQgAABgpP8L5bF+bf28BKQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "#1.Load the data ________________________________________________________________________________________\n",
    "\n",
    "\n",
    "for data_type in ['N3', 'REM', 'N2']:\n",
    "    for segment_length in [1024, 11520, 30720, 61440]:\n",
    "\n",
    "        df_list = joblib.load(data_type + '_normalised_dataframes.pkl')\n",
    "        \n",
    "        print(data_type)    \n",
    "        \n",
    "        # 1. generate all path names and class list(s) etc. \n",
    "        folder = '/user/home/ko20929/work/RBD_using_custom_package/Blue_pebble/'\n",
    "        paths = joblib.load(folder + data_type + '_paths.pkl') # keys : ['selected_paths', 's_class_list', 's_night_list', 's_sleep_type', 's_p_id']\n",
    "        \n",
    "        class_label_dict = {'HC': 0 , 'PD' : 1 , 'PD+RBD' : 2 , 'RBD' : 3} #Dictionary used to label the classes for reference\n",
    "        y = np.array([class_label_dict[c_name] for c_name in paths['s_class_list'] ] )\n",
    "        groups = paths['s_p_id']\n",
    "        \n",
    "        wake_dfs_binary = []\n",
    "        y_binary = []\n",
    "        groups_binary = []\n",
    "        \n",
    "        for df , class_label , group in zip(df_list, y, groups):\n",
    "            if class_label in [0,1]:\n",
    "                wake_dfs_binary.append(df)\n",
    "                y_binary.append(class_label)\n",
    "                groups_binary.append(group)\n",
    "        \n",
    "        y_binary = np.array(y_binary)\n",
    "    \n",
    "    #2. Generate 2 second segments of the data______________________________________________________________________ \n",
    "    \n",
    "        #segment_length is user input and overlap is user input at the start of the script\n",
    "        overlap = 0.5\n",
    "        \n",
    "        signal_slices = []\n",
    "        y_slice_labels = []\n",
    "        y_slice_groups = []\n",
    "        \n",
    "        for df, label, group in zip(wake_dfs_binary, y_binary, groups_binary):\n",
    "            \n",
    "            num_segments = int( np.floor(len(df)/segment_length) )\n",
    "            new_specs = []\n",
    "            \n",
    "            for i in np.arange(0,num_segments, 1 - overlap):\n",
    "                if i > num_segments - 1 :\n",
    "                    # I don't want it to try to take an incomplete slice\n",
    "                    # will be an incomplete slice causing errors downstream, needs to stop \n",
    "                    \n",
    "                    break\n",
    "                  \n",
    "                start_index = int( np.floor(i*segment_length) )\n",
    "                end_index = start_index + segment_length\n",
    "                \n",
    "                slice_df = df.iloc[start_index : end_index, :].copy()        \n",
    "                signal_slices.append(slice_df)\n",
    "                \n",
    "                y_slice_labels.append(label)\n",
    "                y_slice_groups.append(group)\n",
    "                \n",
    "        plt.hist(y_slice_labels)\n",
    "    \n",
    "    #3. Select the correct channels ______________________________________________________________________________\n",
    "        \n",
    "        channels_selected = eeg_14_channels # will later move this to the top of the notebook\n",
    "        \n",
    "        # signal_slices, y_slice_labels, y_slice_groups\n",
    "        signal_slices_14_channels = [ df.loc[:, channels_selected].copy() for df in signal_slices ]\n",
    "        \n",
    "        # Make the slices an np array of the correct dimensions\n",
    "        signal_slices_14_channels_np = [df.T.values for df in signal_slices_14_channels]\n",
    "    \n",
    "    #4. Create the network from the paper (REMOVED as I'm using EEG Conformer) \n",
    "    #The model is defined inside the training loop\n",
    "    \n",
    "    \n",
    "    #5. Do Train Validation Splits\n",
    "        # Train and Validation splits only ----> NO TEST\n",
    "        # spectrogram_slices, y_slice_labels and y_slice_groups to work with\n",
    "        train_val_dict = {}\n",
    "        \n",
    "        for value in ['train' , 'val']:\n",
    "            train_val_dict[value] = {}\n",
    "        \n",
    "        X = np.stack(signal_slices_14_channels_np)\n",
    "        y = np.array(y_slice_labels)\n",
    "        groups = np.array( [int(group) for group in y_slice_groups] )\n",
    "        \n",
    "        gkf = GroupKFold(n_splits = 4) \n",
    "        fold = 0\n",
    "        \n",
    "        for train_index, val_index   in gkf.split(X, y, groups*1):\n",
    "            fold += 1\n",
    "            \n",
    "            X_train, y_train, groups_train  = X[train_index], y[train_index] , groups[train_index]\n",
    "            X_val, y_val, groups_val =  X[val_index], y[val_index] , groups[val_index]   \n",
    "            \n",
    "            train_val_dict['train'][fold] = X_train, y_train, groups_train\n",
    "            train_val_dict['val'][fold]   = X_val, y_val, groups_val\n",
    "            \n",
    "            total_len = len(X) \n",
    "            val_percent = 100*(len(X_val) / total_len)\n",
    "            train_percent = 100*(len(X_train) / total_len)\n",
    "            \n",
    "            # Commented out the printing here\n",
    "            # print('fold ' + str(fold) ) \n",
    "            # print( str(train_percent)[:3] + ' | '  + str(val_percent)[:3] + ' |' )\n",
    "        \n",
    "            # # testing that the splits are as expected\n",
    "            # print( np.unique(groups_train) )\n",
    "            # print( np.unique(groups_val) )\n",
    "            \n",
    "            # print('__________________________________________________________________________')\n",
    "        \n",
    "        #Output from this section of code is X_train, y_train, groups_train AND X_test, y_test, groups_test \n",
    "        import time\n",
    "    \n",
    "        start_time = time.time()\n",
    "        \n",
    "        seeds = [2]\n",
    "        rows = len(seeds) # Make the figure the right size \n",
    "        \n",
    "        fig = plt.figure()\n",
    "        fig = plt.figure(figsize=(24,4*rows),dpi=100)\n",
    "        \n",
    "        # k is for subplots within the overall figure \n",
    "        \n",
    "        k = 1\n",
    "        print('Commencing Training ...')\n",
    "        # Test out for 1 fold to start with\n",
    "        \n",
    "        for fold in [1,2,3,4]:\n",
    "            print(fold)\n",
    "            X_train, y_train, groups_train = train_val_dict['train'][fold]\n",
    "            X_val, y_val, groups_val = train_val_dict['val'][fold]  \n",
    "            \n",
    "            # Creating train and test data loaders\n",
    "            train_data = [ (torch.from_numpy(input_slice).float().view(num_channels,segment_length), val) for input_slice, val in zip(X_train, y_train) ] \n",
    "            train_loader = DataLoader(train_data, batch_size=24, shuffle=True)\n",
    "            \n",
    "            val_data = [ (torch.from_numpy(input_slice).float().view(num_channels,segment_length), val) for input_slice, val in zip(X_val, y_val) ] \n",
    "            val_loader = DataLoader(val_data , batch_size=24, shuffle=False)\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            \n",
    "            # for seed in [2,5,15,50]:\n",
    "            for seed in seeds:\n",
    "                # set all seeds \n",
    "                random.seed(seed)\n",
    "                np.random.seed(seed)\n",
    "                torch.manual_seed(seed)\n",
    "                torch.cuda.manual_seed_all(seed) \n",
    "                \n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                # print(device) - to check that device is actually cuda\n",
    "                \n",
    "                model = EEGConformer(\n",
    "                    n_chans = 14,\n",
    "                    n_outputs = 2 ,\n",
    "                    n_times=segment_length,\n",
    "                    final_fc_length = 'auto' \n",
    "                    )\n",
    "                \n",
    "                model.to(device)\n",
    "                break \n",
    "                \n",
    "                criterion = torch.nn.NLLLoss()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = 0.001 )\n",
    "    \n",
    "                \n",
    "                epochs = 35\n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                test_losses = []\n",
    "                \n",
    "                train_correct = []\n",
    "                val_correct = []\n",
    "                test_correct = []\n",
    "                \n",
    "                for i in range(epochs):\n",
    "                    \n",
    "                    trn_corr = 0\n",
    "                    val_corr = 0\n",
    "                    tst_corr = 0\n",
    "                     \n",
    "                    \n",
    "                    trn_loss = 0\n",
    "                    val_loss = 0\n",
    "                    tst_loss = 0\n",
    "                    \n",
    "                    model.train()\n",
    "                    # Run the training batches\n",
    "                    for b, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "                        b+=1\n",
    "                \n",
    "                        #Move train data to the GPU\n",
    "                        X_train_batch = X_train_batch.to(device)\n",
    "                        y_train_batch = y_train_batch.to(device)\n",
    "                        \n",
    "                        # Apply the model\n",
    "                        y_pred = model(X_train_batch)  # we don't flatten X-train here\n",
    "                        loss = criterion(y_pred, y_train_batch)\n",
    "                 \n",
    "                        # Tally the number of correct predictions\n",
    "                        predicted = torch.argmax(torch.exp( y_pred.detach() ) ,  dim = 1 ) \n",
    "        \n",
    "                        predicted = predicted.reshape(y_train_batch.shape)\n",
    "                        \n",
    "                        batch_corr = (predicted == y_train_batch).sum()\n",
    "                        trn_corr += batch_corr\n",
    "                        trn_loss += loss\n",
    "                        \n",
    "                        # Update parameters\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                    train_losses.append(trn_loss)\n",
    "                    train_correct.append(trn_corr)\n",
    "                \n",
    "                    # Run the validation batches\n",
    "                    # Some of the variables in this loop have the same name as the variables in the above loop... be aware of that plz!\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        for b, (X_val_batch, y_val_batch) in enumerate(val_loader):\n",
    "                            b+=1\n",
    "                            \n",
    "                            #Move train data to the GPU\n",
    "                            X_val_batch = X_val_batch.to(device)\n",
    "                            y_val_batch = y_val_batch.to(device)\n",
    "                \n",
    "                            # Apply the model\n",
    "                            y_val = model(X_val_batch)\n",
    "                \n",
    "                            # Tally the number of correct predictions\n",
    "                            predicted = torch.argmax(y_val.detach(),  dim = 1 ) \n",
    "                            predicted = predicted.reshape(y_val_batch.shape)\n",
    "                            \n",
    "                            batch_corr = (predicted == y_val_batch).sum()\n",
    "                            val_corr += batch_corr\n",
    "                \n",
    "                            loss = criterion(y_val, y_val_batch)\n",
    "                            val_loss += loss \n",
    "                           \n",
    "                    val_losses.append(val_loss)\n",
    "                    val_correct.append(val_corr)\n",
    "                \n",
    "                    \n",
    "                   \n",
    "                \n",
    "                # Plot the outcome from the loop\n",
    "                \n",
    "                ax = fig.add_subplot(rows,4,k)\n",
    "                k+=1\n",
    "                plt.title('fold ' + str(fold), fontsize = 10)\n",
    "                plt.plot([(val.cpu() / len(X_train) ) for val in train_correct], label='training set accuracy')\n",
    "                plt.plot([(val.cpu()/len(X_val) ) for val in val_correct], label='validation set accuracy')\n",
    "                plt.ylabel('accuracy')\n",
    "                plt.xlabel('epochs') \n",
    "                plt.grid()\n",
    "            \n",
    "            \n",
    "            plt.tight_layout()\n",
    "        \n",
    "        \n",
    "        plt.legend()   \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        #Add text at the bottom of the figure\n",
    "        # fig.text(0.5, 0, 'This is a caption at the bottom of the figure | Model : ' + str(model) , va='bottom')\n",
    "        fig.text(0.5, 0, f'\\nDuration: {time.time() - start_time:.0f} seconds' , ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout(pad = 2.0)\n",
    "    \n",
    "        time_taken = str(segment_length/256)\n",
    "        channels_num = str(num_channels)\n",
    "        \n",
    "        save_name = 'Results/Folder_3/' + data_type + '_' + 'Conformer_window_' + time_taken + '_secs_' + channels_num + '_channels' \n",
    "        plt.savefig(save_name +'.png')\n",
    "            \n",
    "        \n",
    "        print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed  \n",
    "        print('Results Saved, on to next data type ...')\n",
    "        \n",
    "t2 = time.time()\n",
    "\n",
    "t2 - t1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
