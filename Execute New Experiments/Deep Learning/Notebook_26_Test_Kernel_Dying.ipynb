{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bfeaee-81d4-407d-bcc1-825462f4189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mne as mne\n",
    "import os \n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import constants\n",
    "from IPython.utils import io\n",
    "import time\n",
    "import sys\n",
    "import yasa\n",
    "from scipy.signal import welch\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from scipy.signal import ShortTimeFFT\n",
    "from scipy.signal.windows import gaussian\n",
    "\n",
    "#Import my modules\n",
    "import format_eeg_data\n",
    "import constants\n",
    "import eeg_stat_ts\n",
    "from constants import region_to_channel_dict, regions\n",
    "\n",
    "#Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#Import Braindecode Model EEG Conformer\n",
    "from braindecode.models import EEGConformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736e1f0-364b-4144-b55d-31f3b86dad5b",
   "metadata": {},
   "source": [
    "### Brain-Region Channels 14 + 20 definitions etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab0cfcfc-25f0-44e6-857e-0bfa074c7ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain_regions_chans = ['Left Frontal', 'Left Frontal', 'Left Frontal', 'Left Central', 'Left Temporal', 'Left Parietal', 'Occipital', 'Occipital', 'Right Parietal', \n",
    " 'Right Temporal', 'Right Central', 'Right Frontal', 'Right Frontal', 'Right Frontal']\n",
    "\n",
    "len(brain_regions_chans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "366814cb-c451-4318-9ba9-51c6411ee4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fpz', 'Fz', 'AF7', 'F7', 'F3', 'F4', 'F8', 'AF8', 'FCz', 'Cz', 'FC5', 'FC6', 'T7', 'T8', 'Pz', 'P7', 'P8', 'O1', 'O2', 'Oz']\n",
      "True\n",
      "['Fpz', 'Fz', 'AF7', 'F7', 'F3', 'F4', 'F8', 'AF8', 'FCz', 'Cz', 'FC5', 'FC6', 'T7', 'T8', 'Pz', 'P7', 'P8', 'O1', 'O2', 'Oz']\n"
     ]
    }
   ],
   "source": [
    "neuro_headset_channels = [\"AF3\", \"F7\", \"F3\", \"FC5\", \"T7\", \"P7\", \"O1\", \"O2\", \"P8\", \"T8\", \"FC6\", \"F4\", \"F8\", \"AF4\"]\n",
    "# These channels are equivalent to the neuro headset channels but for 'our' headset (There is code to generate these in notebook 18) \n",
    "eeg_14_channels = ['AF7', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF8']\n",
    "\n",
    "#No Channels from Prefrontal, Frontal, Central or Parietal - I could add channels from these regions 2 per region = 20 channels\n",
    "#Reasoning: To maintain symmetry between the left and right side of the brain and ensure all regions are represented in the channels:\n",
    "#I will add one channel from Prefrontal 'Fpz', One channel from Frontal 'Fz', Two channels from Central 'FCz' & 'Cz' , One channel from Parietal 'Pz'\n",
    "#And one channel from Occiptal 'Oz'\n",
    "# Channels to add\n",
    "channels_to_add = [ 'Fpz', 'Fz', 'FCz', 'Cz', 'Pz', 'Oz']\n",
    "\n",
    "eeg_20_channels = eeg_14_channels + channels_to_add\n",
    "\n",
    "len(eeg_20_channels)\n",
    "\n",
    "#I want to re-order eeg_20_channels according to Brain Region order in region_to_channel_dict\n",
    "eeg_20_channels_ordered = []\n",
    "for value in region_to_channel_dict.values():\n",
    "    eeg_20_channels_ordered += [channel for channel in eeg_20_channels if channel in value]\n",
    "\n",
    "print(eeg_20_channels_ordered)\n",
    "\n",
    "#Check to see that changing the ordering of the channels didn't negatively effect anything\n",
    "print( sorted(eeg_20_channels) == sorted(eeg_20_channels_ordered) )\n",
    "\n",
    "print(eeg_20_channels_ordered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a41c25-82b1-4014-a42f-f22833a6dbb8",
   "metadata": {},
   "source": [
    "#### Load and re-save dataframes for each data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52525549-857d-459a-8554-6ffa9db7de9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wake\n",
      "N1\n",
      "N2\n",
      "N3\n",
      "REM\n"
     ]
    }
   ],
   "source": [
    "data_types = ['Wake', 'N1', 'N2', 'N3', 'REM']\n",
    "\n",
    "for data_type in data_types:\n",
    "    print(data_type)\n",
    "    df_list = joblib.load(data_type + '_normalised_dataframes.pkl')\n",
    "    \n",
    "    df_list_new = [df[eeg_20_channels_ordered].copy() for df in df_list]\n",
    "    joblib.dump(df_list_new, data_type + '_20_channel_normalised_dataframes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "118062b5-1e8b-4a0c-930b-58cca5defe91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45499999999999996"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.35*1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c7849d5-9355-450f-a7a2-aa1084ff98ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Prefrontal': ['Fp1', 'Fpz', 'Fp2'],\n",
       " 'Frontal': ['F1', 'Fz', 'F2'],\n",
       " 'Left Frontal': ['AF7', 'F7', 'F5', 'F3'],\n",
       " 'Right Frontal': ['AF8', 'F8', 'F6', 'F4'],\n",
       " 'Central': ['FCz', 'FC1', 'FC2', 'Cz', 'C1', 'C2', 'CP1', 'CP2'],\n",
       " 'Left Central': ['FC5', 'FC3', 'C5', 'C3', 'CP5', 'CP3'],\n",
       " 'Right Central': ['FC6', 'FC4', 'C6', 'C4', 'CP6', 'CP4'],\n",
       " 'Left Temporal': ['TP7', 'T7', 'FT7', 'FT9'],\n",
       " 'Right Temporal': ['TP8', 'T8', 'FT8', 'FT10'],\n",
       " 'Parietal': ['P1', 'P2', 'Pz'],\n",
       " 'Left Parietal': ['P3', 'P5', 'P7', 'PO7'],\n",
       " 'Right Parietal': ['P4', 'P6', 'P8', 'PO8'],\n",
       " 'Occipital': ['POz', 'O1', 'O2', 'Oz']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_to_channel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5446cf7-8a22-464a-9a2a-973a7f57c7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Central', 'Frontal', 'Left Central', 'Left Frontal', 'Left Parietal', 'Left Temporal', 'Occipital', 'Parietal', 'Prefrontal', 'Right Central', 'Right Frontal', 'Right Parietal', 'Right Temporal']\n"
     ]
    }
   ],
   "source": [
    "print( sorted(regions) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "340c9f35-2519-4f56-b6e5-f1634d1fafef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Left Central',\n",
       " 'Left Frontal',\n",
       " 'Left Frontal',\n",
       " 'Left Frontal',\n",
       " 'Left Parietal',\n",
       " 'Left Temporal',\n",
       " 'Occipital',\n",
       " 'Occipital',\n",
       " 'Right Central',\n",
       " 'Right Frontal',\n",
       " 'Right Frontal',\n",
       " 'Right Frontal',\n",
       " 'Right Parietal',\n",
       " 'Right Temporal']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorted list\n",
    "sorted_brain_chans = sorted(brain_regions_chans)\n",
    "sorted_brain_chans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df628d31-5f00-475b-bdf5-4c9606abf70c",
   "metadata": {},
   "source": [
    "#### Old code from notebook 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3b6380-55b1-408d-839c-675b2a508193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N1\n",
      "Remember to change n_splits back to 4\n",
      "Commencing Training ...\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/home/ko20929/.conda/envs/sktime_latest/lib/python3.11/site-packages/braindecode/models/base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.09 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 225\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    229\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(trn_loss)\n",
      "File \u001b[0;32m~/.conda/envs/sktime_latest/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sktime_latest/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/sktime_latest/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.09 GiB. GPU "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAGgCAYAAACaOnwjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgoUlEQVR4nO3dbXBU9f338c8hySYbgkLbkApVE0XmAmo1EKUCNdpOQZg/HXBAERRBQEBDBRocCThWHaSlasUE0Q62jNiK9zoyYC0q1AcoJMAkrWkVzULwBoJYNdkNu8C5HjCJLqBkNzn7ZTfv1xNmz549+8uXE/ad3c3iuK7rCgAAwFAX6wUAAAAQJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMylWy+grXbs2CHXdZWRkWG9FAAA0EaRSESO46iwsPA790uaZ0hc1xWf4XaM67oKh8PMIwGYdWIw58RgzonBnKO19fE7pmdIqqur9dRTT2n37t1KT0/XF198oXPOOUdz587V+eef37rfiy++qDVr1sjv9ysUCmnq1KkaPXp07F/FN7Q8M3LhhRe26zipIBgMqra2Vn369FF2drb1clIas04M5pwYzDkxmHO0mpqaNu0XU5Bs2LBB4XBYa9asUVpamg4fPqzbbrtNU6dO1ebNm+U4jl555RXdc889euGFF1RQUKAPPvhA48aNU2ZmpoYPHx7XFwMAAFJbTC/ZjB8/XgsXLlRaWpokKT09XYMHD9a+ffvU2Ngo13X1xz/+UaNHj1ZBQYEk6fzzz9dVV12lBx54oONXDwAAUkJMQXLeeefpBz/4Qevl+vp6Pf/885o0aZK6deum999/Xx999NEJb1wZOHCgAoGA6urqOmbVAAAgpcT1WzabNm3SsmXLVF9fr+nTp+vXv/61JGn37t2SpJ49e0bt33I5EAi0PnMSD9d1FQwG4759qgiFQlF/wjvMOjGYc2Iw58RgztFc15XjOKfcL64gueKKK3TFFVfoww8/VElJiT744AM9/PDDampqkiT5fL6o/VsutzcmIpGIamtr23WMVBIIBKyX0Gkw68RgzonBnBODOX/t+C44mXZ9Dsl5552n0tJSzZ49W2+99Za6du0qSQqHw1H7tVxu77uNMzIy1KdPn3YdIxWEQiEFAgHl5+fL7/dbLyelMevEYM6JwZwTgzlH27VrV5v2iylIwuHwCZVzwQUXSJL+85//qLi4WJK0f//+qH1aLufn58dydydwHIdfofoGv9/PPBKEWScGc04M5pwYzPmYtrxcI8X4ptarrrpKn332WdS2ffv2SZK6d++uCy64QL1799aOHTui9tmxY4fy8/Pb9f4RAACQumL+pNaVK1fqyJEjkqTGxkY9/PDDys3N1fDhw+U4jubNm6d169a1vnb2wQcfaMOGDZo/f36HLhwAAKSOmF6yKS0t1Ysvvqjx48fL7/erqalJ/fr103333aczzzxTkjR69GhFIhHNmzdP2dnZCgaDuvvuuzVixAhPvgAAAJD8YgqSUaNGadSoUafc7+qrr9bVV18d96IAAEDnkjT/uR4AAEhdBAkAADBHkAAAAHMECQAAMEeQADgtZGRktPkDlACknnZ9dDwAdATHcfTj/gPUJT3NeikxcY8eldOFn+uAjkCQADgtdElPU/jJdXL3fXbqnU8DTt735bv+/6yXAaQMggTAacPd95ncj/ZZLwOAAZ5rBAAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAObSY9n5nXfe0dq1a9XQ0CDXddXY2Kjhw4dr2rRpysrKkiSVl5dr48aNOuOMM6JuO2PGDF1++eUdt3IAAJAyYgqSxYsXa+TIkXrwwQflOI4CgYCuueYavffee1q+fHnrfmVlZRo8eHCHLxYAAKSmmF6y6du3r6ZPny7HcSRJ+fn5GjlypF577TU1NTV5skAAAJD6YnqGZMWKFSdsy8rKkuM4SktL67BFfRvXdRUMBj2/n9NdKBSK+hPeYdaJEQ6H5ff7rZcRl1AoJNd1rZfRJpzPicGco7mu2/pExneJKUhOZtu2bRoxYkTre0gk6aWXXlJFRYUOHz6sbt26acyYMRo1alR770qRSES1tbXtPk6qCAQC1kvoNJi1t/x+v7p37269jLjU1dUl3QMP53NiMOev+Xy+U+7TriBZv3699u3bp8cee6x121lnnaXMzEzdfffd8vl8qqys1MyZM1VVVaU777yzPXenjIwM9enTp13HSAWhUEiBQED5+flJ+1NlsmDWiREOh62XELeCgoKkeoaE89l7zDnarl272rRf3EFSXV2tZcuWadWqVcrNzW3dPm7cuKj9ioqKdN1112nVqlWaNWtW1L6xchxH2dnZcd8+1fj9fuaRIMzaW215Ovd0lYwPOJzPicGcj2nr93dcn0NSXV2tBQsWaOXKlerXr98p92/5CWLv3r3x3B0AAEhxMQdJVVWVbr/9dq1YsaI1RjZs2KD6+npJ0rx58064zSeffCJJysvLa89aAQBAioopSN5++22VlJRozpw5CoVCqqmpUU1NjV5++WV9/PHHko69r2T9+vWtt9mzZ4/Wrl2rESNGqFevXh27egAAkBJieg/JvHnzdPDgQc2fP/+E66ZOnSpJuuuuu/TUU0/pySeflCQ1Nzdr8uTJmjJlSvtXCwAAUlJMQbJly5ZT7jNx4kRNnDgx7gUBAIDOh/9cDwAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgLn0WHZ+5513tHbtWjU0NMh1XTU2Nmr48OGaNm2asrKyWvfbvHmzysvLlZmZqaamJo0ZM0ZTpkzp6LUDAIAUEVOQLF68WCNHjtSDDz4ox3EUCAR0zTXX6L333tPy5cslSZWVlbr11lu1evVqFRUVqaGhQWPHjpUkogQAAJxUTC/Z9O3bV9OnT5fjOJKk/Px8jRw5Uq+99pqampokSQ899JAGDx6soqIiSVJubq4mTJig8vJyNTc3d/DyAQBAKogpSFasWKEzzjgjaltWVpYcx1FaWpoaGxtVWVmpwsLCqH0GDhzYeh0AAMDxYnrJ5mS2bdumESNGKCsrS++++65c11XPnj2j9snLy5MkBQIBDRs2LO77cl1XwWCwXetNBaFQKOpPeIdZJ0Y4HJbf77deRlxCoZBc17VeRptwPicGc47mum7rKyvfpV1Bsn79eu3bt0+PPfaYJLXGgs/ni9qv5XJ7YyISiai2trZdx0glgUDAegmdBrP2lt/vV/fu3a2XEZe6urqke+DhfE4M5vy147vgZOIOkurqai1btkyrVq1Sbm6uJCk7O1vSsZ92vqnlcsv18crIyFCfPn3adYxUEAqFFAgElJ+fn7Q/VSYLZp0Yx/+bkUwKCgqS6hkSzmfvMedou3btatN+cQVJdXW1FixYoJUrV6pfv36t28855xw5jqP9+/dH7d9yOT8/P567a+U4TrujJpX4/X7mkSDM2ltteTr3dJWMDzicz4nBnI9p6/d3zB+MVlVVpdtvv10rVqxojZENGzaovr5eOTk5GjRokHbs2BF1m+3btysnJ6f1N28AAAC+KaYgefvtt1VSUqI5c+YoFAqppqZGNTU1evnll/Xxxx9LkubOnautW7eqqqpKknTgwAGtXbtWJSUlUR+eBgAA0CKml2zmzZungwcPav78+SdcN3XqVEnSJZdcooqKCi1durT1k1qnTZvGh6IBAIBvFVOQbNmypU37FRcXq7i4OK4FAQCAzof/XA8AAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJhLj/eGGzdu1L333qvLLrtMv/vd76KuKy8v18aNG3XGGWdEbZ8xY4Yuv/zyeO8SAACkqJiDJBQKqbS0VH6/X5FI5Fv3Kysr0+DBg9u1OAAA0DnE/JJNc3OzJk2apPvvv19ZWVlerAkAAHQyMQdJjx49NGTIEC/WAgAAOqm430NyKi+99JIqKip0+PBhdevWTWPGjNGoUaPadUzXdRUMBjtohckrFApF/QnvMOvECIfD8vv91suISygUkuu61stoE87nxGDO0VzXleM4p9zPkyA566yzlJmZqbvvvls+n0+VlZWaOXOmqqqqdOedd8Z93Egkotra2g5caXILBALWS+g0mLW3/H6/unfvbr2MuNTV1SXdAw/nc2Iw56/5fL5T7uNJkIwbNy7qclFRka677jqtWrVKs2bNUm5ublzHzcjIUJ8+fTpiiUktFAopEAgoPz8/aX+qTBbMOjHC4bD1EuJWUFCQVM+QcD57jzlH27VrV5v28+wlm+O1fNPu3bs37iBxHEfZ2dkdvLLk5ff7mUeCMGtvteXp3NNVMj7gcD4nBnM+pq3f3558MNq8efNO2PbJJ59IkvLy8ry4SwAAkMQ8CZL169dr/fr1rZf37NmjtWvXasSIEerVq5cXdwkAAJJYXC/ZLFq0SHv27FFDQ4Peeust3XDDDRoxYoSuv/56SdJdd92lp556Sk8++aSkY59dMnnyZE2ZMqXDFg4AAFJHXEGyZMmS77x+4sSJmjhxYlwLAgAAnQ//uR4AADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADAXd5Bs3LhRxcXFuuOOO056/ebNmzVu3DhNmjRJY8aM0erVq+O9KwAAkOLSY71BKBRSaWmp/H6/IpHISfeprKzUrbfeqtWrV6uoqEgNDQ0aO3asJGnKlCntWjAAAEg9MT9D0tzcrEmTJun+++9XVlbWSfd56KGHNHjwYBUVFUmScnNzNWHCBJWXl6u5ubl9KwYAACkn5mdIevTooSFDhnzr9Y2NjaqsrFRJSUnU9oEDB6q8vFyVlZUaNmxY7CuV5LqugsFgXLdNJaFQKOpPeIdZJ0Y4HJbf77deRlxCoZBc17VeRptwPicGc47muq4cxznlfjEHyans2bNHruuqZ8+eUdvz8vIkSYFAIO4giUQiqq2tbfcaU0UgELBeQqfBrL3l9/vVvXt362XEpa6uLukeeDifE4M5f83n851ynw4PkpZnMI6/85bL7XmGIyMjQ3369Il/cSkiFAopEAgoPz8/aX+qTBbMOjHC4bD1EuJWUFCQVM+QcD57jzlH27VrV5v26/Agyc7OlnTiPzAtl1uuj4fjOO26farx+/3MI0GYtbfa8nTu6SoZH3A4nxODOR/T1u/vDv8cknPOOUeO42j//v1R21su5+fnd/RdAgCAJNfhQZKTk6NBgwZpx44dUdu3b9+unJyc1t+8AQAAaOHJJ7XOnTtXW7duVVVVlSTpwIEDWrt2rUpKSr71V4UBAEDnFdd7SBYtWqQ9e/aooaFBb731lm644QaNGDFC119/vSTpkksuUUVFhZYuXarMzEw1NTVp2rRpfCgaAAA4qbiCZMmSJafcp7i4WMXFxfEcHgAAdDL853oAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMpXtx0HfeeUcLFy5U7969o7b/7Gc/08033+zFXQIAgCTmSZBI0tixYzVnzhyvDg8AAFIIL9kAAABzBAkAADDn2Us2O3fu1IwZMxQMBpWenq4hQ4boxhtvVFZWVtzHdF1XwWCwA1eZnEKhUNSf8A6zToxwOCy/32+9jLiEQiG5rmu9jDbhfE4M5hzNdV05jnPK/TwJkm7duikvL08LFixQjx499PHHH2v27Nn6+9//rqeffloZGRlxHTcSiai2traDV5u8AoGA9RI6DWbtLb/fr+7du1svIy51dXVJ98DD+ZwYzPlrPp/vlPt4EiT9+/fXfffd13q5V69e+s1vfqMZM2boH//4h0aNGhXXcTMyMtSnT5+OWmbSCoVCCgQCys/PT9qfKpMFs06McDhsvYS4FRQUJNUzJJzP3mPO0Xbt2tWm/Tx7yeZ4BQUFkqT6+vq4j+E4jrKzsztqSUnP7/czjwRh1t5qy9O5p6tkfMDhfE4M5nxMW7+/PXlT6wMPPHBCeHz66aeSpLy8PC/uEgAAJDFPgmTnzp1avXq1jhw5IklqbGzUihUr1Lt3b/3yl7/04i4BAEAS8+Qlm5kzZ+qZZ57RhAkTlJmZqWAwqAsvvFB/+MMf1LVrVy/uEgAAJDFPgmTYsGEaNmyYF4cGAAApiA9GAwAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5tK9OnBdXZ2WLFmiL7/8UuFwWIWFhSotLVXXrl29uksAAJCkPHmG5PPPP9cNN9ygoqIiPfPMM3ruuee0e/dulZaWenF3AAAgyXkSJGvWrFEoFNJNN90kSUpPT9fs2bP1xhtvaPv27V7cJQAASGKeBMmmTZvUv39/+Xy+1m0XXXSRunTpok2bNnlxlwAAIIl58h6S3bt364orroja5vP51KNHDwUCgbiOGYlE5Lquqqur27/AJOe6rhzH0fvvvy/HcayXk9KYdeI4jiN3yP+TjvS1XkrbpHWRU1Mj13WtV9JmnM+JwZyjRSKRNs3BkyAJBoNRz4608Pl8ampqiuuYLV8Mf7nHZtClC78glQjMOrGcnGzrJcQsmf5N4nxODOYczXEcuyDJzs5WOBw+YXs4HI77t2wKCwvbuywAAHCa8iThzj33XO3fvz9qWzgc1ueff678/Hwv7hIAACQxT4KkuLhY7777btSzJNXV1Tp69KiKi4u9uEsAAJDEPAmSyZMny+/3a/Xq1ZKkw4cPa+XKlbryyis1aNAgL+4SAAAkMcf16C3iH374oZYsWaLGxkYdOnRIF198sRYsWMAntQIAgBN4FiQAAABtxe8lAQAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAc57853roeJs3b1Z5ebkyMzPV1NSkMWPGaMqUKTEdY9asWXrzzTf1+uuv60c/+pE3C00B8cy6rq5Of/3rX/Xvf/9b6enp+uqrrzRgwADNmTNHP/zhDxOz8NNUXV2dlixZoi+//FLhcFiFhYUqLS1t04ckrlq1SuvWrVPXrl0VDoc1d+5cDR06NAGrTj7xzPmLL77Q008/rU2bNik9PV1NTU363ve+p5KSEl100UUJXH3yaM/53OLxxx/XsmXLtHTpUl199dUerjbJuDjtbdu2zR0wYIC7bds213Vdd//+/e7QoUPdv/zlL20+xtNPP+1eeumlbt++fd36+nqPVpr84p11WVmZe+ONN7rBYNB1Xdf96quv3Guvvda98sorW7d1RgcPHnSHDh3qrly50nVd141EIu7UqVPdWbNmnfK2jz76qHv55Ze7Bw4ccF3Xdbds2eL++Mc/dnfu3OnpmpNRvHN+6aWX3KFDh7p79+51Xdd1jx496t57773ugAED3HfffdfzdSeb9pzPLf773/+6Q4cOdfv27es+//zzXi01KfGSTRJ46KGHNHjwYBUVFUmScnNzNWHCBJWXl6u5ufmUt6+vr9fjjz+uWbNmeb3UpBfvrHv37q2ZM2fK7/dLknJycjR58mR99NFH2rp1a0LWfjpas2aNQqGQbrrpJklSenq6Zs+erTfeeEPbt2//1ts1NTXp0Ucf1cSJE/X9739fkvTTn/5UhYWFWr58eULWnkzinXP37t01ZcoU9e7dW5LkOI5mzZqlSCSiV155JSFrTybxzrlFJBLRHXfcodtvv93rpSYlguQ019jYqMrKShUWFkZtHzhwYOt13+Xo0aO64447VFZWpjPPPNPLpSa99sz6lltu0WWXXRa1LTMzU5KUlpbW8YtNEps2bVL//v3l8/lat1100UXq0qWLNm3a9K2327p1q4LB4Al/F4WFhXr77bcVCoW8WnJSinfOxcXFmj59etS2rKwsSccebBEt3jm3qKio0GWXXaaBAwd6uMrkRZCc5vbs2SPXddWzZ8+o7Xl5eZKkQCDwnbf/85//rPPOO0/FxcVeLTFltHfWx9u2bZvOOussXXrppR21xKSze/fuE+bp8/nUo0eP75zn7t27JemkfxdHjhxRfX19h681mcU755PZunWrunTpotGjR3fgClNDe+a8c+dObdq0SbfddpuHK0xuJLCBr776Svv37z/lfmeffbaCwaAkRRX5Ny+3XH8y7733np5//nk999xz7VhtckvUrI9XX1+vZ599VhUVFSccrzMJBoMn/fp9Pp+ampq+9XYt13XE30VnEO+cjxeJRLR8+XLdcsstuuCCCzpyiSkh3jmHQiEtXrxYy5Yt69T/HpwKQWLg1Vdf1eLFi0+53/r165WdnS1JCofDUde1XG65/ngtr1Xec889Mb37O9UkYtbH+9///qdbbrlFixYtOuFlnM4mOzv7hHlKx2b6Xedly3Xt/bvoLOKd8ze1vLw7YMAAlZSUdPQSU0K8c162bJlGjRql/v37e7m8pEeQGBg/frzGjx/fpn0bGxvlOM4JP+W3XM7Pzz/p7Wpra9XY2KiHH364dVtDQ4Mkaf78+crMzFRZWZn69esXx1eQPBIx6286ePCgpk+frmnTpmnMmDGxLjflnHvuuSfMMxwO6/PPP//OeZ577rmSjs3+m/vt379faWlpOvvss71YbtKKd84tjhw5orKyMnXt2lW//e1v5TiORytNbvHO+Z///Kfy8vK0ZcsWSdKhQ4ckSX/605/04osvauzYsfz6rwiS015OTo4GDRqkHTt2RG3fvn27cnJyWn8b5Hg/+clP9Nprr0Vte+GFF7Rw4UI9+OCDfA7JScQ76xYNDQ2aPn26Zs6cqVGjRkmS/vWvf+nLL7/UkCFDPFv36ay4uFhPPPGEwuFw61PV1dXVOnr06He+r+nSSy+V3+/Xzp07o96Ds2PHDg0ePLj1t5lwTLxzlqTDhw+rtLRUubm5WrRokaRjn0/y6quv6tprr/V87ckk3jm//vrrUZf37t2rX/ziF7r55psJkW/gTa1JYO7cudq6dauqqqokSQcOHNDatWtVUlLS+o74Q4cOafTo0SorK7NcatKLd9affvqprr/+ev385z/X2WefrZqaGtXU1OjNN99sPVZnNHnyZPn9fq1evVrSsQe/lStX6sorr9SgQYNa91u4cKFGjx7d+pNj165dNWvWLP3tb3/TwYMHJR17s+X27ds1d+7cRH8Zp7145xwOh3XbbbcpGAzqV7/6Vet5W1VVpXXr1ll8Kae1eOeMtuEZkiRwySWXqKKiQkuXLm399NBp06ZFfXqo67oKhUIn/Qaor69XWVlZ1Es2PXv2VEVFRaK+hKQR76yXLl2qQCCgRx55RI888kjUMTvz6/E9evTQE088oSVLluj111/XoUOHdPHFF2vBggVR+x06dEjNzc1yXbd128yZM5Wenq6pU6cqJydH4XBYK1eu5BNETyLeOT/77LPauHGjpGOfUPxNnfm3w75Ne87nFrNnz9Znn30m6euXbH7/+9+rV69eCfkaTmeOe7KJAQAAJBAv2QAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABz/x/0zG3g1LjK+wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "#1.Load the data ________________________________________________________________________________________\n",
    "\n",
    "\n",
    "for data_type in [ 'N1']:\n",
    "    for segment_length in [30720, 61440]:\n",
    "    # for segment_length in [15360]:\n",
    "    \n",
    "        df_list = joblib.load(data_type + '_normalised_dataframes.pkl')\n",
    "        \n",
    "        print(data_type)    \n",
    "        \n",
    "        # 1. generate all path names and class list(s) etc. \n",
    "        folder = '/user/home/ko20929/work/RBD_using_custom_package/Blue_pebble/'\n",
    "        paths = joblib.load(folder + data_type + '_paths.pkl') # keys : ['selected_paths', 's_class_list', 's_night_list', 's_sleep_type', 's_p_id']\n",
    "        \n",
    "        class_label_dict = {'HC': 0 , 'PD' : 1 , 'PD+RBD' : 2 , 'RBD' : 3} #Dictionary used to label the classes for reference\n",
    "        y = np.array([class_label_dict[c_name] for c_name in paths['s_class_list'] ] )\n",
    "        groups = paths['s_p_id']\n",
    "        \n",
    "        wake_dfs_binary = []\n",
    "        y_binary = []\n",
    "        groups_binary = []\n",
    "        \n",
    "        for df , class_label , group in zip(df_list, y, groups):\n",
    "            if class_label in [0,1]:\n",
    "                wake_dfs_binary.append(df)\n",
    "                y_binary.append(class_label)\n",
    "                groups_binary.append(group)\n",
    "        \n",
    "        y_binary = np.array(y_binary)\n",
    "    \n",
    "    #2. Generate 2 second segments of the data______________________________________________________________________ \n",
    "    \n",
    "        #segment_length is user input and overlap is user input at the start of the script\n",
    "        overlap = 0.5\n",
    "        \n",
    "        signal_slices = []\n",
    "        y_slice_labels = []\n",
    "        y_slice_groups = []\n",
    "        \n",
    "        for df, label, group in zip(wake_dfs_binary[:5], y_binary[:5], groups_binary[:5]):\n",
    "            \n",
    "            num_segments = int( np.floor(len(df)/segment_length) )\n",
    "            new_specs = []\n",
    "            \n",
    "            for i in np.arange(0,num_segments, 1 - overlap):\n",
    "                if i > num_segments - 1 :\n",
    "                    # I don't want it to try to take an incomplete slice\n",
    "                    # will be an incomplete slice causing errors downstream, needs to stop \n",
    "                    \n",
    "                    break\n",
    "                  \n",
    "                start_index = int( np.floor(i*segment_length) )\n",
    "                end_index = start_index + segment_length\n",
    "                \n",
    "                slice_df = df.iloc[start_index : end_index, :].copy()        \n",
    "                signal_slices.append(slice_df)\n",
    "                \n",
    "                y_slice_labels.append(label)\n",
    "                y_slice_groups.append(group)\n",
    "                \n",
    "        plt.hist(y_slice_labels)\n",
    "    \n",
    "    #3. Select the correct channels ______________________________________________________________________________\n",
    "        \n",
    "        channels_selected = eeg_14_channels # will later move this to the top of the notebook\n",
    "        \n",
    "        # signal_slices, y_slice_labels, y_slice_groups\n",
    "        signal_slices_14_channels = [ df.loc[:, channels_selected].copy() for df in signal_slices ]\n",
    "        \n",
    "        # Make the slices an np array of the correct dimensions\n",
    "        signal_slices_14_channels_np = [df.T.values for df in signal_slices_14_channels]\n",
    "    \n",
    "    #4. Create the network from the paper (REMOVED as I'm using EEG Conformer) \n",
    "    #The model is defined inside the training loop\n",
    "    \n",
    "    \n",
    "    #5. Do Train Validation Splits\n",
    "        # Train and Validation splits only ----> NO TEST\n",
    "        # spectrogram_slices, y_slice_labels and y_slice_groups to work with\n",
    "        train_val_dict = {}\n",
    "        \n",
    "        for value in ['train' , 'val']:\n",
    "            train_val_dict[value] = {}\n",
    "        \n",
    "        X = np.stack(signal_slices_14_channels_np)\n",
    "        y = np.array(y_slice_labels)\n",
    "        groups = np.array( [int(group) for group in y_slice_groups] )\n",
    "        \n",
    "        gkf = GroupKFold(n_splits = 2)\n",
    "        \n",
    "        print('Remember to change n_splits back to 4')\n",
    "        \n",
    "        fold = 0\n",
    "        \n",
    "        for train_index, val_index   in gkf.split(X, y, groups*1):\n",
    "            fold += 1\n",
    "            \n",
    "            X_train, y_train, groups_train  = X[train_index], y[train_index] , groups[train_index]\n",
    "            X_val, y_val, groups_val =  X[val_index], y[val_index] , groups[val_index]   \n",
    "            \n",
    "            train_val_dict['train'][fold] = X_train, y_train, groups_train\n",
    "            train_val_dict['val'][fold]   = X_val, y_val, groups_val\n",
    "            \n",
    "            total_len = len(X) \n",
    "            val_percent = 100*(len(X_val) / total_len)\n",
    "            train_percent = 100*(len(X_train) / total_len)\n",
    "            \n",
    "            # Commented out the printing here\n",
    "            # print('fold ' + str(fold) ) \n",
    "            # print( str(train_percent)[:3] + ' | '  + str(val_percent)[:3] + ' |' )\n",
    "        \n",
    "            # # testing that the splits are as expected\n",
    "            # print( np.unique(groups_train) )\n",
    "            # print( np.unique(groups_val) )\n",
    "            \n",
    "            # print('__________________________________________________________________________')\n",
    "        \n",
    "        #Output from this section of code is X_train, y_train, groups_train AND X_test, y_test, groups_test \n",
    "        import time\n",
    "    \n",
    "        start_time = time.time()\n",
    "        \n",
    "        seeds = [2]\n",
    "        rows = len(seeds) # Make the figure the right size \n",
    "        \n",
    "        fig = plt.figure()\n",
    "        fig = plt.figure(figsize=(24,4*rows),dpi=100)\n",
    "        \n",
    "        # k is for subplots within the overall figure \n",
    "        \n",
    "        k = 1\n",
    "        print('Commencing Training ...')\n",
    "        # Test out for 1 fold to start with\n",
    "        \n",
    "        for fold in [1,2,3,4]:\n",
    "            print(fold)\n",
    "            X_train, y_train, groups_train = train_val_dict['train'][fold]\n",
    "            X_val, y_val, groups_val = train_val_dict['val'][fold]  \n",
    "            \n",
    "            # Creating train and test data loaders\n",
    "            train_data = [ (torch.from_numpy(input_slice).float().view(num_channels,segment_length), val) for input_slice, val in zip(X_train, y_train) ] \n",
    "            train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "            \n",
    "            val_data = [ (torch.from_numpy(input_slice).float().view(num_channels,segment_length), val) for input_slice, val in zip(X_val, y_val) ] \n",
    "            val_loader = DataLoader(val_data , batch_size=16, shuffle=False)\n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "            \n",
    "            # for seed in [2,5,15,50]:\n",
    "            for seed in seeds:\n",
    "                # set all seeds \n",
    "                random.seed(seed)\n",
    "                np.random.seed(seed)\n",
    "                torch.manual_seed(seed)\n",
    "                torch.cuda.manual_seed_all(seed) \n",
    "                \n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                # print(device) - to check that device is actually cuda\n",
    "                \n",
    "                model = EEGConformer(\n",
    "                    n_chans = num_channels,\n",
    "                    n_outputs = 2 ,\n",
    "                    n_times=segment_length,\n",
    "                    att_depth=3,\n",
    "                    att_heads=5,\n",
    "                    final_fc_length = 'auto' \n",
    "                    )\n",
    "                \n",
    "                model.to(device)\n",
    "                \n",
    "                criterion = torch.nn.NLLLoss()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr = 0.001 )\n",
    "    \n",
    "                \n",
    "                epochs = 35\n",
    "                train_losses = []\n",
    "                val_losses = []\n",
    "                test_losses = []\n",
    "                \n",
    "                train_correct = []\n",
    "                val_correct = []\n",
    "                test_correct = []\n",
    "                \n",
    "                for i in range(epochs):\n",
    "                    \n",
    "                    trn_corr = 0\n",
    "                    val_corr = 0\n",
    "                    tst_corr = 0\n",
    "                     \n",
    "                    \n",
    "                    trn_loss = 0\n",
    "                    val_loss = 0\n",
    "                    tst_loss = 0\n",
    "                    \n",
    "                    model.train()\n",
    "                    # Run the training batches\n",
    "                    for b, (X_train_batch, y_train_batch) in enumerate(train_loader):\n",
    "                        # b+=1\n",
    "                \n",
    "                        #Move train data to the GPU\n",
    "                        X_train_batch = X_train_batch.to(device)\n",
    "                        y_train_batch = y_train_batch.to(device)\n",
    "                        \n",
    "                        # Apply the model\n",
    "                        print(b)\n",
    "                        y_pred = model(X_train_batch)  # we don't flatten X-train here\n",
    "                        loss = criterion(y_pred, y_train_batch)\n",
    "                 \n",
    "                        # Tally the number of correct predictions\n",
    "                        predicted = torch.argmax(torch.exp( y_pred.detach() ) ,  dim = 1 ) \n",
    "        \n",
    "                        predicted = predicted.reshape(y_train_batch.shape)\n",
    "                        \n",
    "                        batch_corr = (predicted == y_train_batch).sum()\n",
    "                        trn_corr += batch_corr\n",
    "                        trn_loss += loss.item()\n",
    "                        \n",
    "                        # Update parameters\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        \n",
    "                    train_losses.append(trn_loss)\n",
    "                    train_correct.append(trn_corr)\n",
    "                \n",
    "                    # Run the validation batches\n",
    "                    # Some of the variables in this loop have the same name as the variables in the above loop... be aware of that plz!\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        for b, (X_val_batch, y_val_batch) in enumerate(val_loader):\n",
    "                            # b+=1\n",
    "                            \n",
    "                            #Move train data to the GPU\n",
    "                            X_val_batch = X_val_batch.to(device)\n",
    "                            y_val_batch = y_val_batch.to(device)\n",
    "                \n",
    "                            # Apply the model\n",
    "                            y_val = model(X_val_batch)\n",
    "                \n",
    "                            # Tally the number of correct predictions\n",
    "                            predicted = torch.argmax(y_val.detach(),  dim = 1 ) \n",
    "                            predicted = predicted.reshape(y_val_batch.shape)\n",
    "                            \n",
    "                            batch_corr = (predicted == y_val_batch).sum()\n",
    "                            val_corr += batch_corr\n",
    "                \n",
    "                            loss = criterion(y_val, y_val_batch)\n",
    "                            val_loss += loss.item()\n",
    "                           \n",
    "                    val_losses.append(val_loss)\n",
    "                    val_correct.append(val_corr)\n",
    "                \n",
    "                    \n",
    "                   \n",
    "                \n",
    "                # Plot the outcome from the loop\n",
    "                \n",
    "                ax = fig.add_subplot(rows,4,k)\n",
    "                k+=1\n",
    "                plt.title('fold ' + str(fold), fontsize = 10)\n",
    "                plt.plot([(val.cpu() / len(X_train) ) for val in train_correct], label='training set accuracy')\n",
    "                plt.plot([(val.cpu()/len(X_val) ) for val in val_correct], label='validation set accuracy')\n",
    "                plt.ylabel('accuracy')\n",
    "                plt.xlabel('epochs') \n",
    "                plt.grid()\n",
    "            \n",
    "            \n",
    "            plt.tight_layout()\n",
    "        \n",
    "        \n",
    "        plt.legend()   \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        #Add text at the bottom of the figure\n",
    "        # fig.text(0.5, 0, 'This is a caption at the bottom of the figure | Model : ' + str(model) , va='bottom')\n",
    "        fig.text(0.5, 0, f'\\nDuration: {time.time() - start_time:.0f} seconds' , ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout(pad = 2.0)\n",
    "    \n",
    "        time_segment = str(segment_length/256)\n",
    "        channels_num = str(num_channels)\n",
    "        \n",
    "        save_name = 'Results/Folder_3/' + data_type + '_' + 'Conformer_window_' + time_segment + '_secs_' + channels_num + '_channels' \n",
    "        plt.savefig(save_name +'.png')\n",
    "            \n",
    "        \n",
    "        print(f'\\nDuration: {time.time() - start_time:.0f} seconds') # print the time elapsed  \n",
    "        print('Results Saved, on to next data type ...')\n",
    "        \n",
    "t2 = time.time()\n",
    "\n",
    "t2 - t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99f8bd-c838-4f4e-b688-385ebff1a5c6",
   "metadata": {},
   "source": [
    "Wake 45 seconds took 260 seconds to run <br> \n",
    "Then it got an out of memory error --> OutOfMemoryError: CUDA out of memory. Tried to allocate 1.55 GiB. GPU \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
